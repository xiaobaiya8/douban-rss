import json
import requests
from bs4 import BeautifulSoup
import time
import os
import re
import random

# è·å–é…ç½®ç›®å½•
CONFIG_DIR = os.getenv('CONFIG_DIR', 'config')
CONFIG_FILE = os.path.join(CONFIG_DIR, 'config.json')
MOVIES_FILE = os.path.join(CONFIG_DIR, 'movies.json')

def get_subject_info(subject_id, cookie, max_retries=3):
    """è·å–æ¡ç›®ä¿¡æ¯"""
    for attempt in range(max_retries):
        try:
            url = f'https://movie.douban.com/subject/{subject_id}/'
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Cookie': cookie
            }
            
            print(f"æ­£åœ¨è·å–æ¡ç›® {subject_id} çš„ä¿¡æ¯...")
            
            # éšæœºå»¶è¿Ÿ1-3ç§’
            delay = random.uniform(1, 3)
            time.sleep(delay)
            
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è¾“å‡ºåŸå§‹æ•°æ®ï¼ŒæŸ¥çœ‹é¡µé¢ç»“æ„
                print("\n=== é¡µé¢ä¿¡æ¯ ===")
                info_div = soup.find('div', {'id': 'info'})
                if info_div:
                    print(info_div.prettify())
                else:
                    print("æœªæ‰¾åˆ°info div")
                
                # è·å–IMDB ID - æ–¹æ³•1ï¼šä»é“¾æ¥è·å–
                imdb_link = soup.find('a', {'href': re.compile(r'www\.imdb\.com/title/(tt\d+)')})
                imdb_id = None
                if imdb_link:
                    print("\n=== IMDBé“¾æ¥ ===")
                    print(imdb_link.prettify())
                    match = re.search(r'tt\d+', imdb_link['href'])
                    if match:
                        imdb_id = match.group()
                        print(f"ä»é“¾æ¥æ‰¾åˆ°IMDB ID: {imdb_id}")
                
                # è·å–IMDB ID - æ–¹æ³•2ï¼šä»æ–‡æœ¬è·å–
                if not imdb_id and info_div:
                    # å…ˆæ‰¾åˆ°IMDb:æ ‡ç­¾
                    imdb_span = info_div.find('span', text=re.compile(r'IMDb:'))
                    if imdb_span:
                        # è·å–IMDbæ ‡ç­¾åé¢çš„æ–‡æœ¬
                        imdb_text = imdb_span.next_sibling
                        if imdb_text:
                            match = re.search(r'tt\d+', imdb_text.strip())
                            if match:
                                imdb_id = match.group()
                                print(f"ä»æ–‡æœ¬æ‰¾åˆ°IMDB ID: {imdb_id}")
                
                # æŸ¥æ‰¾"è°åœ¨çœ‹"æ ‡é¢˜æ¥åˆ¤æ–­ç±»å‹
                others_interests = soup.find('div', {'id': 'subject-others-interests'})
                if others_interests:
                    title = others_interests.find('i')
                    if title:
                        title_text = title.text.strip()
                        print(f"\n=== é¡µé¢æ ‡é¢˜ ===")
                        print(title_text)
                        # æ ¹æ®æ ‡é¢˜åˆ¤æ–­ç±»å‹
                        if 'è¿™éƒ¨å‰§é›†' in title_text:
                            media_type = 'tv'
                        elif 'è¿™éƒ¨ç”µå½±' in title_text:
                            media_type = 'movie'
                        else:
                            # å¦‚æœæ— æ³•ä»æ ‡é¢˜åˆ¤æ–­ï¼Œåˆ™æ£€æŸ¥ç±»å‹æ ‡ç­¾
                            genre_span = info_div.find('span', text='ç±»å‹:') if info_div else None
                            if genre_span:
                                genre = genre_span.find_next('span', property='v:genre')
                                if genre and genre.text.strip() in ['çœŸäººç§€', 'çºªå½•ç‰‡', 'ç»¼è‰º']:
                                    media_type = 'tv'
                                else:
                                    media_type = 'movie'
                            else:
                                media_type = 'movie'  # é»˜è®¤ä¸ºç”µå½±
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°"è°åœ¨çœ‹"åŒºåŸŸï¼Œä½¿ç”¨ç±»å‹æ ‡ç­¾åˆ¤æ–­
                    genre_span = info_div.find('span', text='ç±»å‹:') if info_div else None
                    if genre_span:
                        genre = genre_span.find_next('span', property='v:genre')
                        if genre and genre.text.strip() in ['çœŸäººç§€', 'çºªå½•ç‰‡', 'ç»¼è‰º']:
                            media_type = 'tv'
                        else:
                            media_type = 'movie'
                    else:
                        media_type = 'movie'  # é»˜è®¤ä¸ºç”µå½±
                    
                print(f"\næ¡ç›® {subject_id} çš„ç±»å‹ä¸º: {media_type}")
                if imdb_id:
                    print(f"IMDB IDä¸º: {imdb_id}")
                
                return {
                    'type': media_type,
                    'imdb_id': imdb_id
                }
            else:
                print(f"è·å–æ¡ç›® {subject_id} å¤±è´¥: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                else:
                    raise requests.RequestException(f"è·å–æ¡ç›®ä¿¡æ¯å¤±è´¥: HTTP {response.status_code}")
                    
        except Exception as e:
            print(f"è·å–æ¡ç›® {subject_id} æ—¶å‡ºé”™: {e}")
            if attempt < max_retries - 1:
                print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                time.sleep(3)
                continue
            else:
                raise
    
    return {
        'type': 'movie',  # é»˜è®¤ä¸ºç”µå½±
        'imdb_id': None
    }

def extract_subject_id(url):
    """ä»URLä¸­æå–è±†ç“£ID"""
    match = re.search(r'subject/(\d+)', url)
    return match.group(1) if match else None

def load_cache():
    """åŠ è½½ç¼“å­˜æ•°æ®"""
    try:
        if os.path.exists('movies.json'):
            with open('movies.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
                # æ„å»ºç¼“å­˜å­—å…¸ {title: {type, imdb_id}}
                cache = {}
                for item in data.get('movies', []) + data.get('tv_shows', []):
                    cache[item['title']] = {
                        'type': item['type'],
                        'imdb_id': item['imdb_id'],
                        'info': item['info']
                    }
                print(f"å·²åŠ è½½ç¼“å­˜æ•°æ®: {len(cache)} æ¡è®°å½•")
                return cache
    except Exception as e:
        print(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
    return {}

def parse_movie_item(item, cache, cookie):
    """è§£æç”µå½±æ¡ç›®"""
    # è·å–æ ‡é¢˜
    title_elem = item.find('li', class_='title')
    if not title_elem or not title_elem.find('em'):
        return None
        
    # æ‹†åˆ†ä¸»æ ‡é¢˜å’Œå‰¯æ ‡é¢˜
    full_title = title_elem.find('em').text.strip()
    titles = full_title.split(' / ', 1)
    title = titles[0].strip()  # ä¸»æ ‡é¢˜
    subtitle = titles[1].strip() if len(titles) > 1 else ''  # å‰¯æ ‡é¢˜
    
    # æ£€æŸ¥ç¼“å­˜
    if title in cache:
        print(f"\nä»ç¼“å­˜è·å–: {title}")
        cached_data = cache[title]
        media_type = cached_data['type']
        imdb_id = cached_data['imdb_id']
    else:
        print(f"\næœªæ‰¾åˆ°ç¼“å­˜ï¼Œéœ€è¦è¯·æ±‚: {title}")
        # è·å–é“¾æ¥å’ŒID
        url = ''
        link_elem = item.find('a', href=True)
        if link_elem:
            url = link_elem['href']
        
        # ä»URLä¸­æå–IDå¹¶è·å–ç±»å‹
        subject_id = extract_subject_id(url)
        if subject_id:
            print(f"å¤„ç†æ¡ç›®: {title} (ID: {subject_id})")
            info = get_subject_info(subject_id, cookie)
            media_type = info['type']
            imdb_id = info['imdb_id']
        else:
            print(f"è­¦å‘Š: æ— æ³•ä»URL {url} ä¸­æå–ID")
            media_type = 'movie'
            imdb_id = None
    
    # è·å–å…¶ä»–ä¿¡æ¯
    intro_elem = item.find('li', class_='intro')
    info = intro_elem.text.strip().split(' / ') if intro_elem else []
    
    date_elem = item.find('span', class_='date')
    date = date_elem.text.strip() if date_elem else ''
    
    img_elem = item.find('img')
    cover_url = img_elem.get('src', '') if img_elem else ''
    
    url = ''
    link_elem = item.find('a', href=True)
    if link_elem:
        url = link_elem['href']
    
    playable = bool(item.find('span', class_='playable'))
    
    subject_id = extract_subject_id(url)
    
    return {
        "title": title,
        "subtitle": subtitle,
        "full_title": full_title,
        "info": info,
        "date_added": date,
        "cover_url": cover_url,
        "url": url,
        "playable": playable,
        "type": media_type,
        "id": subject_id,
        "imdb_id": imdb_id
    }

def load_all_data():
    """åŠ è½½æ‰€æœ‰ç”¨æˆ·çš„æ•°æ®"""
    try:
        if os.path.exists(MOVIES_FILE):
            with open(MOVIES_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
    return {}

def save_all_data(all_data):
    """ä¿å­˜æ‰€æœ‰æ•°æ®åˆ°æ–‡ä»¶"""
    # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
    os.makedirs(CONFIG_DIR, exist_ok=True)
    with open(MOVIES_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

def is_duplicate(title, user_id, all_data):
    """æ£€æŸ¥æ¡ç›®æ˜¯å¦é‡å¤"""
    user_data = all_data.get(user_id, {'movies': [], 'tv_shows': []})
    all_items = user_data['movies'] + user_data['tv_shows']
    
    # åŒæ—¶æ£€æŸ¥ä¸»æ ‡é¢˜å’Œå®Œæ•´æ ‡é¢˜
    for item in all_items:
        if title == item.get('title') or title == item.get('full_title'):
            return True
    return False

def generate_movies_json(html_content, user_id, all_data, cookie):
    """è§£æHTMLå†…å®¹å¹¶ç”ŸæˆJSONæ•°æ®"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # è·å–æˆ–åˆå§‹åŒ–ç”¨æˆ·æ•°æ®
    user_data = all_data.get(user_id, {'movies': [], 'tv_shows': []})
    movies = user_data['movies']
    tv_shows = user_data['tv_shows']
    
    # æ‰¾åˆ°ç”µå½±åˆ—è¡¨å®¹å™¨
    items = soup.find_all('div', class_='item')
    total_items = len(items)
    new_items = 0
    
    print(f"\nå¼€å§‹å¤„ç† {total_items} ä¸ªæ¡ç›®...")
    
    for index, item in enumerate(items, 1):
        # è·å–æ ‡é¢˜
        title_elem = item.find('li', class_='title')
        if title_elem and title_elem.find('em'):
            full_title = title_elem.find('em').text.strip()
            titles = full_title.split(' / ', 1)
            title = titles[0].strip()
            
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if is_duplicate(title, user_id, all_data):
                print(f"\næ¡ç›®å·²å­˜åœ¨ï¼Œè·³è¿‡: {title}")
                continue
                
            print(f"\nå¤„ç†æ–°æ¡ç›®: {title}")
            new_items += 1
            data = parse_movie_item(item, {}, cookie)
            
            # è®¾ç½®notifiedä¸ºFalseï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæ–°æ¡ç›®éœ€è¦é€šçŸ¥
            data['notified'] = False
            
            # æ ¹æ®ç±»å‹æ·»åŠ åˆ°å¯¹åº”åˆ—è¡¨
            if data["type"] == "movie":
                movies.append(data)
                print(f"å·²æ·»åŠ åˆ°ç”µå½±åˆ—è¡¨: {data['title']}")
            else:
                tv_shows.append(data)
                print(f"å·²æ·»åŠ åˆ°ç”µè§†å‰§åˆ—è¡¨: {data['title']}")
            
            # å®æ—¶ä¿å­˜æ•°æ®
            all_data[user_id] = {'movies': movies, 'tv_shows': tv_shows}
            save_all_data(all_data)
            print("æ•°æ®å·²ä¿å­˜")
            
            # åªåœ¨å¤„ç†æ–°æ¡ç›®æ—¶æ·»åŠ å»¶è¿Ÿ
            if index < total_items:
                delay = random.uniform(1, 3)
                print(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                time.sleep(delay)
    
    print(f"\nå¤„ç†å®Œæˆ! å‘ç° {new_items} ä¸ªæ–°æ¡ç›®")
    print(f"å½“å‰ç”¨æˆ·å…±æœ‰ {len(movies)} éƒ¨ç”µå½±, {len(tv_shows)} éƒ¨ç”µè§†å‰§")
    
    # æ·»åŠ æ›´æ–°æ—¶é—´å’Œæ–°æ¡ç›®æ•°é‡
    result = {
        'movies': movies,
        'tv_shows': tv_shows,
        'update_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'new_items_count': new_items
    }
    
    return result

def get_douban_html(user_id, cookie):
    """è·å–è±†ç“£HTMLå†…å®¹"""
    url = f'https://movie.douban.com/people/{user_id}/wish'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Cookie': cookie
    }
    
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.text
    else:
        raise requests.RequestException(f"è·å–HTMLå¤±è´¥: HTTP {response.status_code}")

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    return {
        "cookie": "",
        "update_interval": 3600
    }

def fetch_user_data(user_id, cookie):
    """è·å–ç”¨æˆ·æ•°æ®"""
    try:
        print(f"\nå¤„ç†ç”¨æˆ· {user_id} çš„æ•°æ®...")
        
        # è·å–HTMLå†…å®¹
        html_file = f'douban_{user_id}.html'
        if os.path.exists(html_file):
            os.remove(html_file)
            print(f"å·²åˆ é™¤æ—§çš„HTMLæ–‡ä»¶: {html_file}")
        
        print("ä»ç½‘é¡µè·å–æ•°æ®...")
        html_content = get_douban_html(user_id, cookie)
        
        # ä¿å­˜HTMLå†…å®¹åˆ°æ–‡ä»¶
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
            print(f"å·²ä¿å­˜æ–°çš„HTMLæ–‡ä»¶: {html_file}")
        
        print("å¼€å§‹è§£ææ•°æ®...")
        all_data = load_all_data()
        user_data = generate_movies_json(html_content, user_id, all_data, cookie)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°
        has_updates = user_data['new_items_count'] > 0
        
        user_data['has_updates'] = has_updates
        all_data[user_id] = user_data
        save_all_data(all_data)
        
        if has_updates:
            print(f"ç”¨æˆ· {user_id} æ•°æ®æœ‰æ›´æ–°")
        else:
            print(f"ç”¨æˆ· {user_id} æ•°æ®æ— å˜åŒ–")
        
        return has_updates
        
    except Exception as e:
        print(f"å¤„ç†ç”¨æˆ· {user_id} æ—¶å‡ºé”™: {e}")
        raise

def send_telegram_message(message, config, has_new_content=False):
    """å‘é€ Telegram æ¶ˆæ¯"""
    if not config.get('telegram', {}).get('enabled'):
        return
    
    bot_token = config['telegram']['bot_token']
    chat_id = config['telegram']['chat_id']
    notify_mode = config['telegram'].get('notify_mode', 'always')
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€æ¶ˆæ¯ï¼ˆåŸºäºé€šçŸ¥æ¨¡å¼ï¼‰
    if notify_mode == 'new_only' and not has_new_content:
        print("æ²¡æœ‰æ–°å†…å®¹ï¼Œæ ¹æ®é€šçŸ¥è®¾ç½®è·³è¿‡å‘é€æ¶ˆæ¯")
        return
    
    # æ£€æŸ¥ bot_token å’Œ chat_id æ˜¯å¦æœ‰æ•ˆ
    if not bot_token or bot_token == 'your_bot_token_here' or \
       not chat_id or chat_id == 'your_chat_id_here':
        print("Telegram é…ç½®æ— æ•ˆï¼Œè·³è¿‡å‘é€æ¶ˆæ¯")
        return
    
    try:
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        data = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "HTML"
        }
        response = requests.post(url, json=data, timeout=10)  # æ·»åŠ è¶…æ—¶è®¾ç½®
        
        if response.status_code == 200:
            print("Telegram æ¶ˆæ¯å‘é€æˆåŠŸ")
        else:
            error_msg = response.json().get('description', 'æœªçŸ¥é”™è¯¯')
            print(f"å‘é€ Telegram æ¶ˆæ¯å¤±è´¥: {error_msg}")
            
            # å¦‚æœæ˜¯è®¤è¯é”™è¯¯ï¼Œç»™å‡ºæ›´è¯¦ç»†çš„æç¤º
            if response.status_code == 401:
                print("Bot Token æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ˜¯å¦æ­£ç¡®é…ç½®")
            elif response.status_code == 404:
                print("Bot Token æ ¼å¼é”™è¯¯æˆ–å·²å¤±æ•ˆ")
            elif response.status_code == 400:
                print("Chat ID æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ˜¯å¦æ­£ç¡®é…ç½®")
                
    except requests.exceptions.Timeout:
        print("å‘é€ Telegram æ¶ˆæ¯è¶…æ—¶")
    except requests.exceptions.RequestException as e:
        print(f"å‘é€ Telegram æ¶ˆæ¯å‡ºé”™: {e}")
    except Exception as e:
        print(f"å‘é€ Telegram æ¶ˆæ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    try:
        # åˆ é™¤æ‰€æœ‰ douban_*.html æ–‡ä»¶
        for file in os.listdir():
            if file.startswith('douban_') and file.endswith('.html'):
                os.remove(file)
                print(f"å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file}")
    except Exception as e:
        print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def check_cookie_valid(cookie):
    """æ£€æŸ¥ cookie æ˜¯å¦æœ‰æ•ˆ"""
    try:
        # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„è±†ç“£è¯·æ±‚æ¥æµ‹è¯• cookie
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Cookie': cookie
        }
        
        # å°è¯•è®¿é—®è±†ç“£ä¸»é¡µ
        response = requests.get('https://www.douban.com', headers=headers, timeout=10)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
        if 'login' in response.url or response.status_code == 403:
            print("\nâŒ Cookie å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
            return False
            
        print("\nâœ… Cookie éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"\nâŒ éªŒè¯ Cookie æ—¶å‡ºé”™: {e}")
        return False

def main():
    try:
        config = load_config()
        cookie = config.get('cookie', '')
        
        # éªŒè¯ cookie
        if not cookie:
            message = "âŒ Cookie æœªé…ç½®ï¼Œè¯·å…ˆé…ç½® Cookie"
            print(message)
            send_telegram_message(message, config, False)
            return
            
        if not check_cookie_valid(cookie):
            message = "âŒ Cookie å·²å¤±æ•ˆï¼Œè¯·æ›´æ–° Cookie"
            print(message)
            send_telegram_message(message, config, False)
            return
        
        print("\nå¼€å§‹è·å–è±†ç“£æƒ³çœ‹æ•°æ®...")
        
        # è®°å½•æ˜¯å¦æœ‰ä»»ä½•ç”¨æˆ·æ•°æ®æ›´æ–°
        any_updates = False
        # è·å–æ‰€æœ‰æ•°æ®
        all_data = load_all_data()
        
        # å¤„ç†æ¯ä¸ªç”¨æˆ·çš„æ•°æ®
        users = config.get('users', [])
        total_users = len(users)
        
        for i, user in enumerate(users, 1):
            user_id = user['id']
            note = user.get('note', '')
            try:
                print(f"\n[{i}/{total_users}] å¤„ç†ç”¨æˆ·: {note or user_id}")
                
                # è·å–æ›´æ–°
                has_updates = fetch_user_data(user_id, cookie)
                
                if has_updates:
                    any_updates = True
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªç”¨æˆ·ï¼Œæ·»åŠ éšæœºå»¶è¿Ÿ
                if i < total_users:
                    delay = random.uniform(1, 3)  # 1-3ç§’çš„éšæœºå»¶è¿Ÿ
                    print(f"ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªç”¨æˆ·...")
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"å¤„ç†ç”¨æˆ· {user_id} æ—¶å‡ºé”™: {e}")
                continue
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼Œä½†æ ¹æ®æƒ…å†µå†³å®šæ˜¯å¦å‘é€
        # æ— è®ºæ˜¯å¦æœ‰æ›´æ–°ï¼Œéƒ½æ„å»ºæ¶ˆæ¯
        message = "ğŸ¬ <b>è±†ç“£æƒ³çœ‹æ›´æ–°æé†’</b>\n\n"
        
        # é‡æ–°åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œç¡®ä¿è·å–æœ€æ–°çš„æ•°æ®
        all_data = load_all_data()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªé€šçŸ¥çš„æ¡ç›®
        has_unnotified_items = False
        for user_id, user_data in all_data.items():
            movies = user_data.get('movies', [])
            tv_shows = user_data.get('tv_shows', [])
            unnotified_items = [item for item in movies + tv_shows if not item.get('notified', False)]
            if unnotified_items:
                has_unnotified_items = True
                break
        
        if not has_unnotified_items:
            # æ²¡æœ‰æ–°å†…å®¹æ—¶çš„æ¶ˆæ¯
            message += "âš ï¸ æœ¬æ¬¡æ›´æ–°æ²¡æœ‰å‘ç°æ–°çš„æƒ³çœ‹å†…å®¹ã€‚\n\n"
        
        message += f"æ›´æ–°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        if has_unnotified_items:
            # æ•´ç†æ¯ä¸ªç”¨æˆ·çš„æœªé€šçŸ¥æ¡ç›®
            for user in users:
                user_id = user['id']
                note = user.get('note', '')
                user_data = all_data.get(user_id, {})
                movies = user_data.get('movies', [])
                tv_shows = user_data.get('tv_shows', [])
                
                # è·å–æœªé€šçŸ¥çš„æ¡ç›®
                unnotified_items = [item for item in movies + tv_shows if not item.get('notified', False)]
                
                if unnotified_items:
                    message += f"ç”¨æˆ· {note or user_id} æ–°å¢æƒ³çœ‹:\n"
                    
                    for item in unnotified_items:
                        # æ ‡è®°ä¸ºå·²é€šçŸ¥
                        item['notified'] = True
                        
                        # è·å–å¹´ä»½ä¿¡æ¯
                        year = item.get('year', '')
                        
                        # ä½¿ç”¨ä¸»æ ‡é¢˜å’Œå‰¯æ ‡é¢˜ï¼Œæ·»åŠ é“¾æ¥
                        url = item.get('url', f"https://movie.douban.com/subject/{item.get('id', '')}/")
                        message += f"â€¢ <a href=\"{url}\">{item['title']}"
                        if item.get('subtitle'):
                            message += f" ({item['subtitle']})"
                        if year:
                            message += f" [{year}]"
                        message += "</a>\n"
                    
                    message += "\n"
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®
            save_all_data(all_data)
        else:
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            total_movies = sum(len(user_data.get('movies', [])) for user_data in all_data.values())
            total_tv_shows = sum(len(user_data.get('tv_shows', [])) for user_data in all_data.values())
            
            message += f"å½“å‰æ€»è®¡è¿½è¸ª:\n"
            message += f"â€¢ {total_movies} éƒ¨ç”µå½±\n"
            message += f"â€¢ {total_tv_shows} éƒ¨å‰§é›†\n"
        
        # å‘é€ Telegram é€šçŸ¥
        send_telegram_message(message, config, has_unnotified_items)
        
        print("\næ•°æ®è·å–å®Œæˆï¼")
        
    except Exception as e:
        error_message = f"âŒ è·å–è±†ç“£æƒ³çœ‹æ•°æ®æ—¶å‡ºé”™: {str(e)}"
        print(error_message)
        send_telegram_message(error_message, config, False)
    finally:
        # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        cleanup_temp_files()

if __name__ == "__main__":
    main() 