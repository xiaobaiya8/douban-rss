import json
import requests
from bs4 import BeautifulSoup
import time
import os
import re
import random

# è·å–é…ç½®ç›®å½•
CONFIG_DIR = os.getenv('CONFIG_DIR', 'config')
CONFIG_FILE = os.path.join(CONFIG_DIR, 'config.json')
NEW_MOVIES_FILE = os.path.join(CONFIG_DIR, 'new_movies.json')

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    return {
        "cookie": "",
        "update_interval": 3600
    }

def load_new_data():
    """åŠ è½½ç°æœ‰çš„æœ€æ–°ç”µå½±æ•°æ®"""
    try:
        if os.path.exists(NEW_MOVIES_FILE):
            with open(NEW_MOVIES_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥: {e}")
    return {'movies': [], 'tv_shows': [], 'update_time': ''}

def get_douban_new_movies(cookie):
    """è·å–è±†ç“£æœ€æ–°ç”µå½±æ•°æ®"""
    url = 'https://movie.douban.com/j/search_subjects'
    params = {
        'type': 'movie',
        'tag': 'æœ€æ–°',
        'page_limit': 50,
        'page_start': 0
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Cookie': cookie
    }
    
    response = requests.get(url, params=params, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        raise requests.RequestException(f"è·å–æœ€æ–°ç”µå½±æ•°æ®å¤±è´¥: HTTP {response.status_code}")

def get_douban_new_tv(cookie):
    """è·å–è±†ç“£æœ€æ–°ç”µè§†å‰§æ•°æ®"""
    url = 'https://movie.douban.com/j/search_subjects'
    params = {
        'type': 'tv',
        'tag': 'æœ€æ–°',
        'page_limit': 50,
        'page_start': 0
    }
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Cookie': cookie
    }
    
    response = requests.get(url, params=params, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        raise requests.RequestException(f"è·å–æœ€æ–°ç”µè§†å‰§æ•°æ®å¤±è´¥: HTTP {response.status_code}")

def parse_new_movies(cookie):
    """è§£ææœ€æ–°ç”µå½±å’Œç”µè§†å‰§æ•°æ®"""
    # åŠ è½½ç°æœ‰æ•°æ®
    data = load_new_data()
    
    # åˆå§‹åŒ–è®¡æ•°å™¨
    new_movies = 0
    new_tv_shows = 0
    
    # è·å–æœ€æ–°ç”µå½±æ•°æ®
    movies_data = get_douban_new_movies(cookie)
    if movies_data and 'subjects' in movies_data:
        print(f"æ‰¾åˆ° {len(movies_data['subjects'])} ä¸ªæœ€æ–°ç”µå½±")
        new_items = []
        
        # é¢„å…ˆæ£€æŸ¥å“ªäº›æ˜¯æ–°æ¡ç›®
        for item in movies_data['subjects']:
            is_exists = False
            for existing_item in data['movies'] + data['tv_shows']:
                if item['id'] == existing_item['id']:
                    print(f"æ¡ç›®å·²å­˜åœ¨: {item['title']} (ID: {item['id']})")
                    is_exists = True
                    break
            if not is_exists:
                new_items.append(item)
                
        print(f"å…¶ä¸­æœ‰ {len(new_items)} ä¸ªæ–°ç”µå½±")
        
        # å¤„ç†æ–°æ¡ç›®
        for item in new_items:
            result = parse_api_item(item, cookie)
            if result:
                if result['type'] == 'movie':
                    data['movies'].append(result)
                    new_movies += 1
                elif result['type'] == 'tv':
                    data['tv_shows'].append(result)
                    new_tv_shows += 1
                
                # æ›´æ–°æ—¶é—´
                data['update_time'] = time.strftime('%Y-%m-%d %H:%M:%S')
                # ç«‹å³ä¿å­˜æ•°æ®
                save_new_data(data)
                print(f"å·²ä¿å­˜æ–°æ¡ç›®: {result['title']}")
                    
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            delay = random.uniform(3, 7)
            print(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
            time.sleep(delay)
    
    # è·å–æœ€æ–°ç”µè§†å‰§æ•°æ®
    tv_data = get_douban_new_tv(cookie)
    if tv_data and 'subjects' in tv_data:
        print(f"æ‰¾åˆ° {len(tv_data['subjects'])} ä¸ªæœ€æ–°ç”µè§†å‰§")
        new_items = []
        
        # é¢„å…ˆæ£€æŸ¥å“ªäº›æ˜¯æ–°æ¡ç›®
        for item in tv_data['subjects']:
            is_exists = False
            for existing_item in data['movies'] + data['tv_shows']:
                if item['id'] == existing_item['id']:
                    print(f"æ¡ç›®å·²å­˜åœ¨: {item['title']} (ID: {item['id']})")
                    is_exists = True
                    break
            if not is_exists:
                new_items.append(item)
                
        print(f"å…¶ä¸­æœ‰ {len(new_items)} ä¸ªæ–°ç”µè§†å‰§")
        
        # å¤„ç†æ–°æ¡ç›®
        for item in new_items:
            result = parse_api_item(item, cookie)
            if result:
                if result['type'] == 'tv':
                    data['tv_shows'].append(result)
                    new_tv_shows += 1
                elif result['type'] == 'movie':
                    data['movies'].append(result)
                    new_movies += 1
                
                # æ›´æ–°æ—¶é—´
                data['update_time'] = time.strftime('%Y-%m-%d %H:%M:%S')
                # ç«‹å³ä¿å­˜æ•°æ®
                save_new_data(data)
                print(f"å·²ä¿å­˜æ–°æ¡ç›®: {result['title']}")
                    
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            delay = random.uniform(3, 7)
            print(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
            time.sleep(delay)
    
    if new_movies == 0 and new_tv_shows == 0:
        print("æ²¡æœ‰å‘ç°æ–°çš„å†…å®¹ï¼Œæ— éœ€æ›´æ–°")
        data['has_updates'] = False
    else:
        print(f"æ–°å¢ {new_movies} éƒ¨æœ€æ–°ç”µå½±å’Œ {new_tv_shows} éƒ¨æœ€æ–°ç”µè§†å‰§")
        data['has_updates'] = True
    
    return data

def save_new_data(data):
    """ä¿å­˜æœ€æ–°æ•°æ®åˆ°æ–‡ä»¶"""
    os.makedirs(CONFIG_DIR, exist_ok=True)
    with open(NEW_MOVIES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def check_cookie_valid(cookie):
    """æ£€æŸ¥ cookie æ˜¯å¦æœ‰æ•ˆ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Cookie': cookie
        }
        
        response = requests.get('https://www.douban.com', headers=headers, timeout=10)
        
        if 'login' in response.url or response.status_code == 403:
            print("\nâŒ Cookie å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
            return False
            
        print("\nâœ… Cookie éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"\nâŒ éªŒè¯ Cookie æ—¶å‡ºé”™: {e}")
        return False

def send_telegram_message(message, config):
    """å‘é€ Telegram æ¶ˆæ¯"""
    if not config.get('telegram', {}).get('enabled'):
        return
    
    bot_token = config['telegram']['bot_token']
    chat_id = config['telegram']['chat_id']
    
    if not bot_token or bot_token == 'your_bot_token_here' or \
       not chat_id or chat_id == 'your_chat_id_here':
        print("Telegram é…ç½®æ— æ•ˆï¼Œè·³è¿‡å‘é€æ¶ˆæ¯")
        return
    
    try:
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        data = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "HTML"
        }
        response = requests.post(url, json=data, timeout=10)
        
        if response.status_code == 200:
            print("Telegram æ¶ˆæ¯å‘é€æˆåŠŸ")
        else:
            print(f"å‘é€ Telegram æ¶ˆæ¯å¤±è´¥: {response.json().get('description', 'æœªçŸ¥é”™è¯¯')}")
            
    except Exception as e:
        print(f"å‘é€ Telegram æ¶ˆæ¯æ—¶å‡ºé”™: {e}")

def get_subject_info(subject_id, cookie, max_retries=3):
    """è·å–æ¡ç›®ä¿¡æ¯"""
    for attempt in range(max_retries):
        try:
            url = f'https://movie.douban.com/subject/{subject_id}/'
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Cookie': cookie
            }
            
            print(f"æ­£åœ¨è·å–æ¡ç›® {subject_id} çš„ä¿¡æ¯...")
            
            # éšæœºå»¶è¿Ÿ2-5ç§’
            delay = random.uniform(2, 5)
            time.sleep(delay)
            
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # åˆå§‹åŒ–å…¶ä»–ä¿¡æ¯
                info = {
                    'type': 'movie',  # é»˜è®¤ä¸ºç”µå½±
                    'imdb_id': None,
                    'year': '',
                    'duration': '',
                    'region': '',
                    'director': [],
                    'actors': [],
                    'genres': [],
                    'languages': [],
                    'release_date': '',
                    'vote_count': '',
                    'episodes_info': ''
                }
                
                # åˆ¤æ–­ç±»å‹ï¼ˆç”µå½±/ç”µè§†å‰§ï¼‰
                # 1. æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é›†çŸ­è¯„
                episode_comments = soup.find('i', text=re.compile(r'çš„åˆ†é›†çŸ­è¯„'))
                if episode_comments:
                    info['type'] = 'tv'
                else:
                    # 2. æ£€æŸ¥æ˜¯å¦æœ‰é›†æ•°ä¿¡æ¯
                    episode_info = soup.find('span', text=re.compile(r'é›†æ•°:|å…±\d+é›†'))
                    episode_duration = soup.find('span', text=re.compile(r'å•é›†ç‰‡é•¿:'))
                    if episode_info or episode_duration:
                        info['type'] = 'tv'
                        if episode_info and episode_info.next_sibling:
                            info['episodes_info'] = episode_info.next_sibling.strip()
                
                info_div = soup.find('div', {'id': 'info'})
                if info_div:
                    # è·å–IMDB ID
                    imdb_link = soup.find('a', {'href': re.compile(r'www\.imdb\.com/title/(tt\d+)')})
                    if imdb_link:
                        match = re.search(r'tt\d+', imdb_link['href'])
                        if match:
                            info['imdb_id'] = match.group()
                    
                    if not info['imdb_id']:
                        imdb_span = info_div.find('span', text=re.compile(r'IMDb:'))
                        if imdb_span and imdb_span.next_sibling:
                            imdb_text = imdb_span.next_sibling.strip()
                            match = re.search(r'tt\d+', imdb_text)
                            if match:
                                info['imdb_id'] = match.group()
                    
                    # è·å–å¹´ä»½
                    year_span = soup.find('span', {'class': 'year'})
                    if year_span:
                        year_match = re.search(r'\d{4}', year_span.text)
                        if year_match:
                            info['year'] = year_match.group()
                    
                    # è·å–å¯¼æ¼”
                    director_span = info_div.find('span', text='å¯¼æ¼”')
                    if director_span:
                        director_links = director_span.find_next('span')
                        if director_links:
                            info['director'] = [link.text.strip() for link in director_links.find_all('a')]
                    
                    # è·å–ä¸»æ¼”
                    actors_span = info_div.find('span', text='ä¸»æ¼”')
                    if actors_span:
                        actor_links = actors_span.find_next('span')
                        if actor_links:
                            info['actors'] = [link.text.strip() for link in actor_links.find_all('a')]
                    
                    # è·å–ç±»å‹
                    genre_spans = info_div.find_all('span', {'property': 'v:genre'})
                    if genre_spans:
                        info['genres'] = [span.text.strip() for span in genre_spans]
                    
                    # è·å–åˆ¶ç‰‡å›½å®¶/åœ°åŒº
                    region_text = info_div.find(text=re.compile(r'åˆ¶ç‰‡å›½å®¶/åœ°åŒº:'))
                    if region_text and region_text.next_sibling:
                        info['region'] = region_text.next_sibling.strip()
                    
                    # è·å–è¯­è¨€
                    language_text = info_div.find(text=re.compile(r'è¯­è¨€:'))
                    if language_text and language_text.next_sibling:
                        languages = language_text.next_sibling.strip()
                        if languages:
                            info['languages'] = [lang.strip() for lang in languages.split('/') if lang.strip()]
                    
                    # è·å–ç‰‡é•¿
                    duration_span = info_div.find('span', {'property': 'v:runtime'})
                    if duration_span:
                        info['duration'] = duration_span.text.strip()
                    
                    # è·å–ä¸Šæ˜ æ—¥æœŸ
                    release_date_span = info_div.find('span', {'property': 'v:initialReleaseDate'})
                    if release_date_span:
                        info['release_date'] = release_date_span.text.strip()
                
                # è·å–è¯„åˆ†äººæ•°
                votes_span = soup.find('span', {'property': 'v:votes'})
                if votes_span:
                    info['vote_count'] = votes_span.text.strip()
                
                print(f"æ¡ç›® {subject_id} çš„ç±»å‹ä¸º: {info['type']}")
                if info['imdb_id']:
                    print(f"IMDB IDä¸º: {info['imdb_id']}")
                    
                return info
            
            elif response.status_code == 403:
                print(f"è¯·æ±‚è¢«é™åˆ¶ï¼Œç­‰å¾…åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(random.uniform(5, 10))
                continue
            else:
                print(f"è·å–æ¡ç›® {subject_id} å¤±è´¥: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    continue
                return None
                
        except Exception as e:
            print(f"è·å–æ¡ç›® {subject_id} æ—¶å‡ºé”™: {e}")
            if attempt < max_retries - 1:
                continue
            return None
    
    print(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
    return None

def parse_api_item(item, cookie):
    """è§£æAPIè¿”å›çš„æ¡ç›®"""
    try:
        subject_id = item['id']
        
        # è·å–è¯¦ç»†ä¿¡æ¯
        info = get_subject_info(subject_id, cookie)
        if not info:
            return None
            
        return {
            "title": item['title'],
            "rating": item['rate'],
            "vote_count": info['vote_count'],
            "duration": info['duration'],
            "region": info['region'],
            "director": info['director'],
            "actors": info['actors'],
            "cover_url": item['cover'],
            "url": item['url'],
            "id": subject_id,
            "type": info['type'],
            "imdb_id": info['imdb_id'],
            "year": info['year'],
            "genres": info['genres'],
            "languages": info['languages'],
            "release_date": info['release_date'],
            "episodes_info": info['episodes_info']
        }
        
    except Exception as e:
        print(f"è§£ææ¡ç›®æ—¶å‡ºé”™: {e}")
        return None

def extract_subject_id(url):
    """ä»URLä¸­æå–è±†ç“£ID"""
    match = re.search(r'subject/(\d+)', url)
    return match.group(1) if match else None

def main():
    try:
        config = load_config()
        cookie = config.get('cookie', '')
        
        # éªŒè¯ cookie
        if not cookie:
            message = "âŒ Cookie æœªé…ç½®ï¼Œè¯·å…ˆé…ç½® Cookie"
            print(message)
            send_telegram_message(message, config)
            return
            
        if not check_cookie_valid(cookie):
            message = "âŒ Cookie å·²å¤±æ•ˆï¼Œè¯·æ›´æ–° Cookie"
            print(message)
            send_telegram_message(message, config)
            return
        
        print("\nå¼€å§‹è·å–è±†ç“£æœ€æ–°æ•°æ®...")
        
        # è§£ææ•°æ®
        data = parse_new_movies(cookie)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        movies_count = len(data['movies'])
        tv_shows_count = len(data['tv_shows'])
        
        # åªåœ¨æœ‰æ›´æ–°æ—¶æ‰å‘é€æ¶ˆæ¯
        if data.get('has_updates', False):
            # ç”Ÿæˆé€šçŸ¥æ¶ˆæ¯
            message = (
                f"ğŸ¬ <b>è±†ç“£æœ€æ–°æ•°æ®æ›´æ–°å®Œæˆ</b>\n\n"
                f"æ›´æ–°æ—¶é—´: {data['update_time']}\n"
                f"æ€»ç”µå½±æ•°: {movies_count} éƒ¨\n"
                f"æ€»å‰§é›†æ•°: {tv_shows_count} éƒ¨\n\n"
            )
            
            # æ–°å¢å†…å®¹ï¼šæ·»åŠ æ–°æ›´æ–°çš„ç”µå½±ä¿¡æ¯
            if data.get('has_updates', False):
                message += "<b>æ–°å¢ç”µå½±:</b>\n"
                new_movies = [movie for movie in data['movies'] if not movie.get('notified', False)]
                for movie in new_movies[:5]:  # æœ€å¤šæ˜¾ç¤º5éƒ¨æ–°ç”µå½±
                    movie_link = movie.get('url', f"https://movie.douban.com/subject/{movie.get('id', '')}/")
                    message += f"â€¢ <a href='{movie_link}'>{movie['title']}</a> - â­{movie['rating']}\n"
                    movie['notified'] = True  # æ ‡è®°ä¸ºå·²é€šçŸ¥
                    
                # å¦‚æœæ–°ç”µå½±è¶…è¿‡5éƒ¨ï¼Œæ·»åŠ "ç­‰"å­—æ ·
                if len(new_movies) > 5:
                    message += f"ç­‰ {len(new_movies)} éƒ¨æ–°ç”µå½±\n"
                    
                message += "\n"
                
            message += "<b>æœ€æ–°ç”µå½± TOP 5:</b>\n"
            
            # æ·»åŠ æœ€æ–°ç”µå½±ä¿¡æ¯
            for i, movie in enumerate(sorted(data['movies'], key=lambda x: float(x['rating'] or 0), reverse=True)[:5], 1):
                movie_link = movie.get('url', f"https://movie.douban.com/subject/{movie.get('id', '')}/")
                message += f"{i}. <a href='{movie_link}'>{movie['title']}</a> - â­{movie['rating']}\n"
                
            # åªæœ‰åœ¨æœ‰ç”µè§†å‰§æ—¶æ‰æ˜¾ç¤ºç”µè§†å‰§éƒ¨åˆ†
            if tv_shows_count > 0:
                message += "\n<b>æœ€æ–°å‰§é›† TOP 5:</b>\n"
                
                # æ·»åŠ æœ€æ–°å‰§é›†ä¿¡æ¯
                for i, tv in enumerate(sorted(data['tv_shows'], key=lambda x: float(x['rating'] or 0), reverse=True)[:5], 1):
                    tv_link = tv.get('url', f"https://movie.douban.com/subject/{tv.get('id', '')}/")
                    message += f"{i}. <a href='{tv_link}'>{tv['title']}</a> - â­{tv['rating']}\n"
            
            # å‘é€ Telegram é€šçŸ¥
            send_telegram_message(message, config)
        
        print(f"\næ•°æ®è·å–å®Œæˆï¼æ€»è®¡ {movies_count} éƒ¨æœ€æ–°ç”µå½±å’Œ {tv_shows_count} éƒ¨æœ€æ–°ç”µè§†å‰§")
        
    except Exception as e:
        error_message = f"âŒ è·å–è±†ç“£æœ€æ–°æ•°æ®æ—¶å‡ºé”™: {str(e)}"
        print(error_message)
        send_telegram_message(error_message, config)

if __name__ == "__main__":
    main() 