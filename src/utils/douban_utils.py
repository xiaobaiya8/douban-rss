import os
import json
import requests
import time
import re
from typing import Dict, List, Any, Optional, Union, Tuple
from bs4 import BeautifulSoup
import random

# å®šä¹‰å¯å¯¼å‡ºçš„å‡½æ•°åˆ—è¡¨
__all__ = [
    'load_config', 
    'check_cookie_valid',
    'extract_subject_id',
    'get_subject_info',
    'get_api_data',
    'parse_api_item',
    'send_telegram_message',
    'send_wecom_message',
    'make_douban_headers',
    'load_json_data',
    'save_json_data'
]

# æ·»åŠ ç‰ˆæœ¬ä¿¡æ¯
__version__ = '1.0.0'

# è·å–é…ç½®ç›®å½•
CONFIG_DIR = os.getenv('CONFIG_DIR', 'config')
CONFIG_FILE = os.path.join(CONFIG_DIR, 'config.json')

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    return {
        "cookie": "",
        "update_interval": 3600
    }

def check_cookie_valid(cookie):
    """æ£€æŸ¥ cookie æ˜¯å¦æœ‰æ•ˆ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Cookie': cookie
        }
        
        response = requests.get('https://www.douban.com', headers=headers, timeout=10)
        
        if 'login' in response.url or response.status_code == 403:
            print("\nâŒ Cookie å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
            return False
            
        print("\nâœ… Cookie éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"\nâŒ éªŒè¯ Cookie æ—¶å‡ºé”™: {e}")
        return False

def extract_subject_id(url):
    """ä» URL ä¸­æå–è±†ç“£ ID"""
    match = re.search(r'subject/(\d+)', url)
    if match:
        return match.group(1)
    return None

def get_subject_info(subject_id, cookie, max_retries=3):
    """è·å–æ¡ç›®è¯¦ç»†ä¿¡æ¯"""
    for attempt in range(max_retries):
        try:
            url = f'https://movie.douban.com/subject/{subject_id}/'
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Cookie': cookie
            }
            
            print(f"æ­£åœ¨è·å–æ¡ç›® {subject_id} çš„ä¿¡æ¯...")
            
            # éšæœºå»¶è¿Ÿ2-5ç§’ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            delay = random.uniform(2, 5)
            time.sleep(delay)
            
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # åˆå§‹åŒ–æ¡ç›®ä¿¡æ¯
                info = {
                    'type': 'movie',  # é»˜è®¤ä¸ºç”µå½±
                    'imdb_id': None,
                    'year': '',
                    'duration': '',
                    'region': '',
                    'director': [],
                    'actors': [],
                    'genres': [],
                    'languages': [],
                    'release_date': '',
                    'vote_count': '',
                    'episodes_info': {}
                }
                
                # åˆ¤æ–­ç±»å‹ï¼ˆç”µå½±/ç”µè§†å‰§ï¼‰
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é›†çŸ­è¯„ï¼ˆç”µè§†å‰§ç‰¹æœ‰ï¼‰
                is_tv = 'çš„åˆ†é›†çŸ­è¯„' in response.text
                episode_info = soup.find('span', text=re.compile(r'é›†æ•°:|å…±\d+é›†'))
                episode_duration = soup.find(text=re.compile(r'å•é›†ç‰‡é•¿:'))
                
                if is_tv or episode_info or episode_duration:
                    info['type'] = 'tv'
                    if episode_info and episode_info.next_sibling:
                        info['episodes_info']['count'] = episode_info.next_sibling.strip()
                    if episode_duration and episode_duration.next_sibling:
                        info['episodes_info']['duration'] = episode_duration.next_sibling.strip()
                
                info_div = soup.find('div', {'id': 'info'})
                if info_div:
                    # è·å–IMDB ID
                    imdb_link = soup.find('a', {'href': re.compile(r'www\.imdb\.com/title/(tt\d+)')})
                    if imdb_link:
                        match = re.search(r'tt\d+', imdb_link['href'])
                        if match:
                            info['imdb_id'] = match.group()
                    
                    if not info['imdb_id']:
                        imdb_span = info_div.find('span', text=re.compile(r'IMDb:'))
                        if imdb_span and imdb_span.next_sibling:
                            imdb_text = imdb_span.next_sibling.strip()
                            match = re.search(r'tt\d+', imdb_text)
                            if match:
                                info['imdb_id'] = match.group()
                    
                    # è·å–å¹´ä»½
                    year_span = soup.find('span', {'class': 'year'})
                    if year_span:
                        year_match = re.search(r'\d{4}', year_span.text)
                        if year_match:
                            info['year'] = year_match.group()
                    
                    # è·å–å¯¼æ¼”
                    director_span = info_div.find('span', text='å¯¼æ¼”')
                    if director_span:
                        director_links = director_span.find_next('span')
                        if director_links:
                            info['director'] = [link.text.strip() for link in director_links.find_all('a')]
                    
                    # è·å–ä¸»æ¼”
                    actors_span = info_div.find('span', text='ä¸»æ¼”')
                    if actors_span:
                        actor_links = actors_span.find_next('span')
                        if actor_links:
                            info['actors'] = [link.text.strip() for link in actor_links.find_all('a')][:3]  # åªå–å‰ä¸‰ä¸ªä¸»æ¼”
                    
                    # è·å–ç±»å‹
                    genre_spans = info_div.find_all('span', {'property': 'v:genre'})
                    if genre_spans:
                        info['genres'] = [span.text.strip() for span in genre_spans]
                    
                    # è·å–åˆ¶ç‰‡å›½å®¶/åœ°åŒº
                    region_text = info_div.find(text=re.compile(r'åˆ¶ç‰‡å›½å®¶/åœ°åŒº:'))
                    if region_text and region_text.next_sibling:
                        info['region'] = region_text.next_sibling.strip()
                    
                    # è·å–è¯­è¨€
                    language_text = info_div.find(text=re.compile(r'è¯­è¨€:'))
                    if language_text and language_text.next_sibling:
                        languages = language_text.next_sibling.strip()
                        if languages:
                            info['languages'] = [lang.strip() for lang in languages.split('/') if lang.strip()]
                    
                    # è·å–ç‰‡é•¿
                    duration_span = info_div.find('span', {'property': 'v:runtime'})
                    if duration_span:
                        info['duration'] = duration_span.text.strip()
                    
                    # è·å–ä¸Šæ˜ æ—¥æœŸ
                    release_date_span = info_div.find('span', {'property': 'v:initialReleaseDate'})
                    if release_date_span:
                        info['release_date'] = release_date_span.text.strip()
                
                # è·å–è¯„åˆ†äººæ•°
                votes_span = soup.find('span', {'property': 'v:votes'})
                if votes_span:
                    info['vote_count'] = votes_span.text.strip()
                
                print(f"æ¡ç›® {subject_id} çš„ç±»å‹ä¸º: {info['type']}")
                if info['imdb_id']:
                    print(f"IMDB IDä¸º: {info['imdb_id']}")
                    
                return info
            
            elif response.status_code == 403:
                print(f"è¯·æ±‚è¢«é™åˆ¶ï¼Œç­‰å¾…åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(random.uniform(5, 10))
                continue
            elif response.status_code == 404:
                print(f"æ¡ç›® {subject_id} ä¸å­˜åœ¨")
                return None
            else:
                print(f"è·å–æ¡ç›® {subject_id} å¤±è´¥: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(2, 5))
                    continue
                return None
                
        except Exception as e:
            print(f"è·å–æ¡ç›® {subject_id} æ—¶å‡ºé”™: {e}")
            if attempt < max_retries - 1:
                time.sleep(random.uniform(2, 5))
                continue
            return None
    
    print(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
    return None

def get_api_data(type_value, tag_value, cookie, page_limit=50, page_start=0):
    """è·å–è±†ç“£APIæ•°æ®ï¼Œç”¨äºçƒ­é—¨ã€æœ€æ–°ã€å†·é—¨ä½³ç‰‡ç­‰æ ‡ç­¾
    
    å‚æ•°:
        type_value: 'movie' æˆ– 'tv'
        tag_value: 'çƒ­é—¨', 'æœ€æ–°', 'å†·é—¨ä½³ç‰‡' ç­‰æ ‡ç­¾
        cookie: è±†ç“£cookie
        page_limit: æ¯é¡µæ¡ç›®æ•°é‡
        page_start: èµ·å§‹ä½ç½®
    
    è¿”å›:
        APIè¿”å›çš„jsonæ•°æ®
    """
    url = 'https://movie.douban.com/j/search_subjects'
    params = {
        'type': type_value,
        'tag': tag_value,
        'page_limit': page_limit,
        'page_start': page_start
    }
    headers = make_douban_headers(cookie)
    
    response = requests.get(url, params=params, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        raise requests.RequestException(f"è·å–æ•°æ®å¤±è´¥: HTTP {response.status_code}")

def parse_api_item(item, cookie):
    """è§£æ API è¿”å›çš„æ¡ç›®æ•°æ®"""
    try:
        # è·å–è¯¦ç»†ä¿¡æ¯
        subject_id = item['id']
        info = get_subject_info(subject_id, cookie)
        
        if not info:
            return None
            
        # åˆå¹¶åŸºæœ¬ä¿¡æ¯
        result = {
            'id': subject_id,
            'title': item['title'],
            'url': item['url'],
            'cover': item['cover'],
            'rate': item['rate'],
            'rating': item.get('rating', {}).get('value', item['rate']),
            'type': info['type'],
            'imdb_id': info['imdb_id'],
            'year': info['year'],
            'director': info['director'],
            'actors': info['actors'],
            'genres': info['genres'],
            'region': info['region'],
            'languages': info['languages'],
            'duration': info['duration'],
            'release_date': info['release_date'],
            'vote_count': info['vote_count']
        }
        
        # å¦‚æœæ˜¯ç”µè§†å‰§ï¼Œæ·»åŠ å‰§é›†ä¿¡æ¯
        if info['type'] == 'tv':
            result['episodes_info'] = info['episodes_info']
            
        return result
        
    except Exception as e:
        print(f"è§£ææ¡ç›® {item['id']} æ—¶å‡ºé”™: {e}")
        return None

def send_telegram_message(message, config, has_new_content=False):
    """å‘é€ Telegram æ¶ˆæ¯"""
    if not config.get('telegram', {}).get('enabled'):
        return
    
    bot_token = config['telegram']['bot_token']
    chat_id = config['telegram']['chat_id']
    notify_mode = config['telegram'].get('notify_mode', 'always')
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€æ¶ˆæ¯ï¼ˆåŸºäºé€šçŸ¥æ¨¡å¼ï¼‰
    if notify_mode == 'new_only' and not has_new_content:
        print("æ²¡æœ‰æ–°å†…å®¹ï¼Œæ ¹æ®é€šçŸ¥è®¾ç½®è·³è¿‡å‘é€æ¶ˆæ¯")
        return
    
    if not bot_token or bot_token == 'your_bot_token_here' or \
       not chat_id or chat_id == 'your_chat_id_here':
        print("Telegram é…ç½®æ— æ•ˆï¼Œè·³è¿‡å‘é€æ¶ˆæ¯")
        return
    
    try:
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        data = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "HTML"
        }
        
        # è®¾ç½®ä»£ç†
        proxies = None
        if config.get('telegram', {}).get('proxy', {}).get('enabled'):
            proxy_type = config['telegram']['proxy'].get('type', 'http')
            proxy_url = config['telegram']['proxy'].get('url', '')
            
            if proxy_url:
                if proxy_type == 'http':
                    proxies = {
                        'http': f'http://{proxy_url}',
                        'https': f'http://{proxy_url}'
                    }
                elif proxy_type == 'socks5':
                    proxies = {
                        'http': f'socks5://{proxy_url}',
                        'https': f'socks5://{proxy_url}'
                    }
                print(f"ä½¿ç”¨{proxy_type}ä»£ç†: {proxy_url}")
        
        # ä½¿ç”¨ä»£ç†å‘é€è¯·æ±‚
        response = requests.post(url, json=data, proxies=proxies, timeout=10)
        
        if response.status_code == 200:
            print("Telegram æ¶ˆæ¯å‘é€æˆåŠŸ")
        else:
            error_msg = response.json().get('description', 'æœªçŸ¥é”™è¯¯')
            print(f"å‘é€ Telegram æ¶ˆæ¯å¤±è´¥: {error_msg}")
            
            # å¦‚æœæ˜¯è®¤è¯é”™è¯¯ï¼Œç»™å‡ºæ›´è¯¦ç»†çš„æç¤º
            if response.status_code == 401:
                print("Bot Token æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ˜¯å¦æ­£ç¡®é…ç½®")
            elif response.status_code == 404:
                print("Bot Token æ ¼å¼é”™è¯¯æˆ–å·²å¤±æ•ˆ")
            elif response.status_code == 400:
                print("Chat ID æ— æ•ˆï¼Œè¯·æ£€æŸ¥æ˜¯å¦æ­£ç¡®é…ç½®")
                
    except requests.exceptions.Timeout:
        print("å‘é€ Telegram æ¶ˆæ¯è¶…æ—¶")
    except requests.exceptions.RequestException as e:
        print(f"å‘é€ Telegram æ¶ˆæ¯å‡ºé”™: {e}")
    except Exception as e:
        print(f"å‘é€ Telegram æ¶ˆæ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

def send_wecom_message(message, config, has_new_content=False):
    """å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯"""
    wecom_config = config.get('wecom', {})
    if not wecom_config.get('enabled'):
        return
    
    corpid = wecom_config.get('corpid')
    corpsecret = wecom_config.get('corpsecret')
    agentid = wecom_config.get('agentid')
    touser = wecom_config.get('touser', '@all')
    notify_mode = wecom_config.get('notify_mode', 'always')
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€æ¶ˆæ¯ï¼ˆåŸºäºé€šçŸ¥æ¨¡å¼ï¼‰
    if notify_mode == 'new_only' and not has_new_content:
        print("æ²¡æœ‰æ–°å†…å®¹ï¼Œæ ¹æ®é€šçŸ¥è®¾ç½®è·³è¿‡å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯")
        return
    
    if not corpid or not corpsecret or not agentid:
        print("ä¼ä¸šå¾®ä¿¡é…ç½®æ— æ•ˆï¼Œè·³è¿‡å‘é€æ¶ˆæ¯")
        return
    
    try:
        # è®¾ç½®ä»£ç†
        proxies = None
        if wecom_config.get('proxy', {}).get('enabled'):
            proxy_type = wecom_config['proxy'].get('type', 'http')
            proxy_url = wecom_config['proxy'].get('url', '')
            
            if proxy_url:
                if proxy_type == 'http':
                    proxies = {
                        'http': f'http://{proxy_url}',
                        'https': f'http://{proxy_url}'
                    }
                elif proxy_type == 'socks5':
                    proxies = {
                        'http': f'socks5://{proxy_url}',
                        'https': f'socks5://{proxy_url}'
                    }
                print(f"ä¼ä¸šå¾®ä¿¡ä½¿ç”¨{proxy_type}ä»£ç†: {proxy_url}")
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–è®¿é—®ä»¤ç‰Œ
        token_url = f"https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid={corpid}&corpsecret={corpsecret}"
        token_response = requests.get(token_url, proxies=proxies, timeout=10)
        token_data = token_response.json()
        
        if token_data.get('errcode') != 0:
            print(f"è·å–ä¼ä¸šå¾®ä¿¡è®¿é—®ä»¤ç‰Œå¤±è´¥: {token_data.get('errmsg')}")
            return
            
        access_token = token_data.get('access_token')
        
        # ç¬¬äºŒæ­¥ï¼šå‘é€æ¶ˆæ¯
        send_url = f"https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token={access_token}"
        
        # ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼šå°†HTMLæ ¼å¼è½¬ä¸ºçº¯æ–‡æœ¬æ ¼å¼
        # 1. æ›¿æ¢ç‰¹æ®ŠHTMLæ ‡ç­¾ä¸ºä¼ä¸šå¾®ä¿¡æ”¯æŒçš„æ ¼å¼
        clean_message = message
        
        # å¤„ç†<b>å’Œ</b>æ ‡ç­¾ - åŠ ç²—ç”¨æ˜Ÿå·ä»£æ›¿
        clean_message = re.sub(r'<b>(.*?)</b>', r'*\1*', clean_message)
        
        # å¤„ç†<i>å’Œ</i>æ ‡ç­¾ - ä½¿ç”¨ä¸‹åˆ’çº¿è¡¨ç¤º
        clean_message = re.sub(r'<i>(.*?)</i>', r'_\1_', clean_message)
        
        # å¤„ç†<a>æ ‡ç­¾ - è½¬ä¸ºçº¯æ–‡æœ¬é“¾æ¥æ ¼å¼
        clean_message = re.sub(r'<a href=[\'"]([^\'"]*)[\'"]>(.*?)</a>', r'\2 [\1]', clean_message)
        
        # å¤„ç†æ¢è¡Œ
        clean_message = clean_message.replace('<br>', '\n').replace('<br/>', '\n').replace('<br />', '\n')
        
        # å¤„ç†ç‰¹æ®Šç¬¦å·
        clean_message = clean_message.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        clean_message = clean_message.replace('&quot;', '"').replace('&apos;', "'")
        
        # å¤„ç†åˆ—è¡¨é¡¹ï¼ˆé€šå¸¸HTMLä½¿ç”¨<li>æ ‡ç­¾ï¼‰
        clean_message = re.sub(r'<li>(.*?)</li>', r'â€¢ \1', clean_message)
        
        # ç§»é™¤å‰©ä½™çš„HTMLæ ‡ç­¾
        clean_message = re.sub(r'<[^>]+>', '', clean_message)
        
        # å¤„ç†ç‰¹æ®Šemoji/ç¬¦å·ï¼ˆä¿ç•™å®ƒä»¬ï¼‰
        # ğŸ¬ âš ï¸ ğŸ“Š ç­‰ç¬¦å·åº”è¯¥ä¿ç•™
        
        send_data = {
            "touser": touser,
            "msgtype": "text",
            "agentid": agentid,
            "text": {
                "content": clean_message
            }
        }
        
        send_response = requests.post(send_url, json=send_data, proxies=proxies, timeout=10)
        send_result = send_response.json()
        
        if send_result.get('errcode') == 0:
            print("ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯å‘é€æˆåŠŸ")
        else:
            print(f"å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯å¤±è´¥: {send_result.get('errmsg')}")
            
    except requests.exceptions.Timeout:
        print("å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯è¶…æ—¶")
    except requests.exceptions.RequestException as e:
        print(f"å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯å‡ºé”™: {e}")
    except Exception as e:
        print(f"å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

def make_douban_headers(cookie):
    """ç”Ÿæˆå¸¦æœ‰cookieçš„è¯·æ±‚å¤´"""
    return {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Cookie': cookie
    }

def load_json_data(file_path, default_value=None):
    """åŠ è½½JSONæ•°æ®æ–‡ä»¶"""
    if default_value is None:
        default_value = {'movies': [], 'tv_shows': [], 'update_time': ''}
        
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                print(f"è¯»å–æ–‡ä»¶: {file_path}")
                data = json.load(f)
                print(f"è¯»å–æ–‡ä»¶æˆåŠŸ: {file_path}")
                return data
        else:
            print(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼: {file_path}")
    except Exception as e:
        print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
    return default_value

def save_json_data(data, file_path):
    """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"å†™å…¥æ–‡ä»¶æˆåŠŸ: {file_path}")
        return True
    except Exception as e:
        print(f"ä¿å­˜æ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return False 