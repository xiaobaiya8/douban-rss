import json
import requests
from bs4 import BeautifulSoup
import time
import os
import re
import random
import logging
# å¯¼å…¥è±†ç“£å·¥å…·æ¨¡å—
from src.utils.douban_utils import extract_subject_id, load_config, check_cookie_valid, send_telegram_message, send_wecom_message, make_douban_headers, load_json_data, save_json_data, migrate_legacy_cache_data

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('douban_status')

# è·å–é…ç½®ç›®å½•
CONFIG_DIR = os.getenv('CONFIG_DIR', 'config')
CONFIG_FILE = os.path.join(CONFIG_DIR, 'config.json')
STATUS_FILE = os.path.join(CONFIG_DIR, 'status.json')
URL_CACHE_FILE = os.path.join(CONFIG_DIR, 'url_cache.json')  # æ·»åŠ URLç¼“å­˜æ–‡ä»¶

# å…¨å±€ç¼“å­˜å˜é‡
url_cache = {
    'short_urls': {},  # çŸ­é“¾æ¥ -> å®é™…URL
    'trailer_urls': {},  # é¢„å‘Šç‰‡URL -> ç”µå½±/å‰§é›†ä¿¡æ¯
    'subject_types': {},  # æ¡ç›®ID -> ç±»å‹(movie/tv)
    'subject_details': {}  # æ¡ç›®ID -> å®Œæ•´è¯¦æƒ…
}

# å…¨å±€URLç¼“å­˜
URL_CACHE = {}

# ä½¿ç”¨å·²å¯¼å…¥çš„å‡½æ•°æ›¿æ¢åŸæœ‰çš„å¼•ç”¨
def extract_trailer_id(url):
    """ä»é¢„å‘Šç‰‡URLä¸­æå–ID"""
    match = re.search(r'/trailer/(\d+)', url)
    if match:
        return match.group(1)
    return None

def get_subject_info_from_trailer(trailer_id, cookie, max_retries=3):
    """ä»é¢„å‘Šç‰‡é¡µé¢è·å–ç”µå½±/å‰§é›†ä¿¡æ¯ï¼Œä½¿ç”¨ç¼“å­˜"""
    # æ„å»ºé¢„å‘Šç‰‡URL
    trailer_url = f'https://movie.douban.com/trailer/{trailer_id}/'
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    global url_cache
    if trailer_url in url_cache['trailer_urls']:
        cached_info = url_cache['trailer_urls'][trailer_url]
        print(f"ä»ç¼“å­˜è·å–é¢„å‘Šç‰‡ä¿¡æ¯: {trailer_id} -> {cached_info['title']} (ID: {cached_info['id']})")
        return cached_info
    
    # ç¼“å­˜ä¸å­˜åœ¨ï¼Œè¿›è¡Œè¯·æ±‚
    for attempt in range(max_retries):
        try:
            headers = make_douban_headers(cookie)
            
            print(f"æ­£åœ¨ä»é¢„å‘Šç‰‡ {trailer_id} è·å–ç”µå½±ä¿¡æ¯...")
            
            # éšæœºå»¶è¿Ÿ1-3ç§’
            delay = random.uniform(1, 3)
            time.sleep(delay)
            
            response = requests.get(trailer_url, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾ç”µå½±é“¾æ¥
                h1_elem = soup.find('h1')
                if h1_elem:
                    movie_link = h1_elem.find('a')
                    if movie_link and 'href' in movie_link.attrs:
                        movie_url = movie_link['href']
                        subject_id = extract_subject_id(movie_url)
                        title = movie_link.text.strip()
                        
                        print(f"ä»é¢„å‘Šç‰‡æ‰¾åˆ°ç”µå½±: {title} (ID: {subject_id})")
                        
                        # åˆ›å»ºç»“æœå¹¶ä¿å­˜åˆ°ç¼“å­˜
                        result = {
                            'id': subject_id,
                            'title': title,
                            'url': movie_url
                        }
                        url_cache['trailer_urls'][trailer_url] = result
                        return result
                
                print(f"æ— æ³•ä»é¢„å‘Šç‰‡ {trailer_id} è·å–ç”µå½±ä¿¡æ¯")
                return None
                
            else:
                print(f"è·å–é¢„å‘Šç‰‡ {trailer_id} å¤±è´¥: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                else:
                    raise requests.RequestException(f"è·å–é¢„å‘Šç‰‡ä¿¡æ¯å¤±è´¥: HTTP {response.status_code}")
                    
        except Exception as e:
            print(f"è·å–é¢„å‘Šç‰‡ {trailer_id} æ—¶å‡ºé”™: {e}")
            if attempt < max_retries - 1:
                print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                time.sleep(3)
                continue
            else:
                raise
    
    return None

def get_subject_type(subject_id, cookie, max_retries=3):
    """è·å–æ¡ç›®ç±»å‹ï¼ˆç”µå½±æˆ–ç”µè§†å‰§ï¼‰ï¼Œä½¿ç”¨ç¼“å­˜"""
    # å…ˆæ£€æŸ¥ç¼“å­˜
    global url_cache
    if subject_id in url_cache['subject_types']:
        cached_type = url_cache['subject_types'][subject_id]
        print(f"ä»ç¼“å­˜è·å–æ¡ç›®ç±»å‹: {subject_id} -> {cached_type}")
        return cached_type
    
    # ç¼“å­˜ä¸å­˜åœ¨ï¼Œè¿›è¡Œè¯·æ±‚
    for attempt in range(max_retries):
        try:
            url = f'https://movie.douban.com/subject/{subject_id}/'
            headers = make_douban_headers(cookie)
            
            print(f"æ­£åœ¨è·å–æ¡ç›® {subject_id} çš„ç±»å‹...")
            
            # éšæœºå»¶è¿Ÿ1-3ç§’
            delay = random.uniform(1, 3)
            time.sleep(delay)
            
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾"è°åœ¨çœ‹"æ ‡é¢˜æ¥åˆ¤æ–­ç±»å‹
                others_interests = soup.find('div', {'id': 'subject-others-interests'})
                if others_interests:
                    title = others_interests.find('i')
                    if title:
                        title_text = title.text.strip()
                        # æ ¹æ®æ ‡é¢˜åˆ¤æ–­ç±»å‹
                        if 'è¿™éƒ¨å‰§é›†' in title_text:
                            # ä¿å­˜åˆ°ç¼“å­˜
                            url_cache['subject_types'][subject_id] = 'tv'
                            return 'tv'
                        elif 'è¿™éƒ¨ç”µå½±' in title_text:
                            # ä¿å­˜åˆ°ç¼“å­˜
                            url_cache['subject_types'][subject_id] = 'movie'
                            return 'movie'
                
                # å¦‚æœæ— æ³•ä»"è°åœ¨çœ‹"åˆ¤æ–­ï¼Œåˆ™æ£€æŸ¥ç±»å‹æ ‡ç­¾
                info_div = soup.find('div', {'id': 'info'})
                if info_div:
                    genre_span = info_div.find('span', text='ç±»å‹:')
                    if genre_span:
                        genre = genre_span.find_next('span', property='v:genre')
                        if genre and genre.text.strip() in ['çœŸäººç§€', 'çºªå½•ç‰‡', 'ç»¼è‰º']:
                            # ä¿å­˜åˆ°ç¼“å­˜
                            url_cache['subject_types'][subject_id] = 'tv'
                            return 'tv'
                
                # ä»é¡µé¢æ ‡é¢˜åˆ¤æ–­
                title = soup.find('title')
                if title:
                    title_text = title.text.strip()
                    if 'å‰§é›†' in title_text or 'ç”µè§†å‰§' in title_text:
                        # ä¿å­˜åˆ°ç¼“å­˜
                        url_cache['subject_types'][subject_id] = 'tv'
                        return 'tv'
                
                # é»˜è®¤ä¸ºç”µå½±
                # ä¿å­˜åˆ°ç¼“å­˜
                url_cache['subject_types'][subject_id] = 'movie'
                return 'movie'
                
            else:
                print(f"è·å–æ¡ç›® {subject_id} å¤±è´¥: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                else:
                    raise requests.RequestException(f"è·å–æ¡ç›®ä¿¡æ¯å¤±è´¥: HTTP {response.status_code}")
                    
        except Exception as e:
            print(f"è·å–æ¡ç›® {subject_id} æ—¶å‡ºé”™: {e}")
            if attempt < max_retries - 1:
                print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                time.sleep(3)
                continue
            else:
                raise
    
    # é»˜è®¤è¿”å›ç”µå½±å¹¶ä¿å­˜åˆ°ç¼“å­˜
    url_cache['subject_types'][subject_id] = 'movie'
    return 'movie'

def get_redirect_url(short_url, headers, max_retries=3):
    """è·å–çŸ­é“¾æ¥çš„é‡å®šå‘ç›®æ ‡URLï¼Œä½¿ç”¨ç¼“å­˜"""
    # å…ˆæ£€æŸ¥ç¼“å­˜
    global url_cache
    if short_url in url_cache['short_urls']:
        cached_url = url_cache['short_urls'][short_url]
        print(f"ä»ç¼“å­˜è·å–çŸ­é“¾æ¥é‡å®šå‘: {short_url} -> {cached_url}")
        return cached_url
    
    # ç¼“å­˜ä¸å­˜åœ¨ï¼Œè¿›è¡Œè¯·æ±‚
    for attempt in range(max_retries):
        try:
            response = requests.head(short_url, headers=headers, allow_redirects=False, timeout=10)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å®šå‘
            if response.status_code in (301, 302, 303, 307, 308) and 'Location' in response.headers:
                redirect_url = response.headers['Location']
                # ä¿å­˜åˆ°ç¼“å­˜
                url_cache['short_urls'][short_url] = redirect_url
                return redirect_url
            
            # å¦‚æœæ²¡æœ‰é‡å®šå‘ä½†è¿”å›æˆåŠŸï¼Œç›´æ¥è¿”å›åŸURL
            if response.status_code == 200:
                # ä¿å­˜åˆ°ç¼“å­˜
                url_cache['short_urls'][short_url] = short_url
                return short_url
                
            # å…¶ä»–æƒ…å†µï¼Œå°è¯•å®Œæ•´GETè¯·æ±‚
            response = requests.get(short_url, headers=headers, allow_redirects=True, timeout=10)
            final_url = response.url
            # ä¿å­˜åˆ°ç¼“å­˜
            url_cache['short_urls'][short_url] = final_url
            return final_url
                
        except Exception as e:
            print(f"è·å–é‡å®šå‘URLæ—¶å‡ºé”™: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
            else:
                return None
    
    return None

def get_subject_details(subject_id, cookie, max_retries=3):
    """è·å–æ¡ç›®çš„å®Œæ•´è¯¦æƒ…ä¿¡æ¯ï¼Œä½¿ç”¨ç¼“å­˜"""
    # å…ˆæ£€æŸ¥ç¼“å­˜
    global url_cache
    if subject_id in url_cache['subject_details']:
        cached_details = url_cache['subject_details'][subject_id]
        print(f"ä»ç¼“å­˜è·å–æ¡ç›®è¯¦æƒ…: {subject_id} -> {cached_details['title']}")
        return cached_details
    
    # ç¼“å­˜ä¸å­˜åœ¨ï¼Œè¿›è¡Œè¯·æ±‚
    for attempt in range(max_retries):
        try:
            url = f'https://movie.douban.com/subject/{subject_id}/'
            headers = make_douban_headers(cookie)
            
            print(f"æ­£åœ¨è·å–æ¡ç›® {subject_id} çš„å®Œæ•´è¯¦æƒ…...")
            
            # éšæœºå»¶è¿Ÿ1-3ç§’
            delay = random.uniform(1, 3)
            time.sleep(delay)
            
            response = requests.get(url, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # è·å–æ ‡é¢˜
                title = ""
                title_elem = soup.find('span', property='v:itemreviewed')
                if title_elem:
                    title = title_elem.text.strip()
                
                # åˆ†å‰²ä¸»æ ‡é¢˜å’Œå‰¯æ ‡é¢˜
                main_title = title
                subtitle = ""
                full_title = title
                if " " in title and not title.startswith(" "):
                    parts = title.split(" ", 1)
                    main_title = parts[0].strip()
                    subtitle = parts[1].strip()
                
                # è·å–è¯„åˆ†
                rating = ""
                rating_elem = soup.find('strong', property='v:average')
                if rating_elem:
                    rating = rating_elem.text.strip()
                
                # è·å–å¹´ä»½
                year = ""
                year_elem = soup.find('span', class_='year')
                if year_elem:
                    year_text = year_elem.text.strip()
                    year_match = re.search(r'\((\d{4})\)', year_text)
                    if year_match:
                        year = year_match.group(1)
                
                # è·å–æµ·æŠ¥URL
                cover_url = ""
                cover_elem = soup.find('img', rel='v:image')
                if cover_elem and 'src' in cover_elem.attrs:
                    cover_url = cover_elem['src']
                
                # è·å–IMDb ID
                imdb_id = ""
                info_div = soup.find('div', {'id': 'info'})
                if info_div:
                    imdb_link = info_div.find('a', href=re.compile(r'https?://www.imdb.com/title/(tt\d+)'))
                    if imdb_link:
                        imdb_match = re.search(r'tt\d+', imdb_link['href'])
                        if imdb_match:
                            imdb_id = imdb_match.group(0)
                    else:
                        # å°è¯•ä»æ–‡æœ¬ä¸­æå–IMDbä¿¡æ¯
                        imdb_text = info_div.text
                        imdb_match = re.search(r'IMDb:\s*(tt\d+)', imdb_text)
                        if imdb_match:
                            imdb_id = imdb_match.group(1)
                
                # è·å–æ‰€æœ‰ä¿¡æ¯é¡¹
                info = []
                if info_div:
                    # è·å–æ‰€æœ‰ä¿¡æ¯æ–‡æœ¬ï¼Œå¹¶è¿›è¡Œæ¸…ç†
                    info_text = info_div.text.strip().replace('\n', ' ')
                    info_text = re.sub(r'\s+', ' ', info_text)  # å‹ç¼©å¤šä½™ç©ºæ ¼
                    
                    # æå–å¯¼æ¼”
                    director_match = re.search(r'å¯¼æ¼”:\s*([^:]+?)(?:ä¸»æ¼”|ç¼–å‰§|åˆ¶ç‰‡å›½å®¶|åœ°åŒº)', info_text)
                    if director_match:
                        directors = director_match.group(1).strip().split(' / ')
                        for director in directors:
                            if director.strip():
                                info.append(director.strip())
                    
                    # æå–ä¸»æ¼”
                    actor_match = re.search(r'ä¸»æ¼”:\s*([^:]+?)(?:ç±»å‹|ä¸Šæ˜ æ—¥æœŸ|ç‰‡é•¿)', info_text)
                    if actor_match:
                        actors = actor_match.group(1).strip().split(' / ')
                        for actor in actors[:8]:  # é™åˆ¶ä¸»æ¼”æ•°é‡
                            if actor.strip():
                                info.append(actor.strip())
                    
                    # æå–åˆ¶ç‰‡å›½å®¶/åœ°åŒº
                    region_match = re.search(r'åˆ¶ç‰‡å›½å®¶/åœ°åŒº:\s*([^:]+?)(?:è¯­è¨€|ç±»å‹|ä¸Šæ˜ æ—¥æœŸ)', info_text)
                    if region_match:
                        regions = region_match.group(1).strip().split(' / ')
                        for region in regions:
                            if region.strip():
                                info.append(region.strip())
                    
                    # æå–å¯¼æ¼”ï¼ˆå†æ¬¡å°è¯•ï¼Œä»¥ç¡®ä¿èƒ½å¤Ÿæå–åˆ°ï¼‰
                    if len(info) < 3:
                        director_spans = info_div.find_all('span', class_='attrs')
                        for span in director_spans:
                            prev_span = span.find_previous('span')
                            if prev_span and 'å¯¼æ¼”' in prev_span.text:
                                directors = span.text.strip().split(' / ')
                                for director in directors:
                                    if director.strip() and director.strip() not in info:
                                        info.append(director.strip())
                    
                    # æå–ä¸Šæ˜ æ—¥æœŸ
                    date_spans = info_div.find_all('span', property='v:initialReleaseDate')
                    if date_spans:
                        dates = []
                        for date_span in date_spans:
                            date_text = date_span.text.strip()
                            if date_text:
                                dates.append(date_text)
                        if dates:
                            # ä¼˜å…ˆé€‰æ‹©ä¸­å›½å¤§é™†ä¸Šæ˜ æ—¥æœŸ
                            cn_date = None
                            for date in dates:
                                if 'ä¸­å›½å¤§é™†' in date:
                                    cn_date = date
                                    break
                            info.insert(0, cn_date if cn_date else dates[0])
                    
                    # æå–ç‰‡é•¿
                    duration_span = info_div.find('span', property='v:runtime')
                    if duration_span:
                        duration = duration_span.text.strip()
                        if duration:
                            info.append(duration)
                    
                    # æå–ç±»å‹
                    genre_spans = info_div.find_all('span', property='v:genre')
                    if genre_spans:
                        genres = []
                        for genre_span in genre_spans:
                            genre = genre_span.text.strip()
                            if genre:
                                genres.append(genre)
                        if genres:
                            info.append('/'.join(genres))
                    
                    # æå–è¯­è¨€
                    language_match = re.search(r'è¯­è¨€:\s*([^:]+?)(?:ç±»å‹|ä¸Šæ˜ æ—¥æœŸ|ç‰‡é•¿|å®˜æ–¹ç½‘ç«™|åˆ¶ç‰‡å›½å®¶)', info_text)
                    if language_match:
                        languages = language_match.group(1).strip().split(' / ')
                        if languages:
                            info.append(languages[0].strip())
                
                # è·å–è¯¦æƒ…æè¿°
                description = ""
                desc_span = soup.find('span', property='v:summary')
                if desc_span:
                    description = desc_span.text.strip()
                
                # æ„å»ºç»“æœ
                result = {
                    'id': subject_id,
                    'title': main_title,
                    'subtitle': subtitle,
                    'full_title': full_title,
                    'info': info,
                    'year': year,
                    'description': description,
                    'cover_url': cover_url,
                    'url': url,
                    'rating': rating,
                    'playable': True,
                    'imdb_id': imdb_id if imdb_id else None
                }
                
                # ç¡®å®šç±»å‹
                media_type = "movie"
                others_interests = soup.find('div', {'id': 'subject-others-interests'})
                if others_interests:
                    title = others_interests.find('i')
                    if title and 'è¿™éƒ¨å‰§é›†' in title.text.strip():
                        media_type = "tv"
                
                result['type'] = media_type
                
                # ä¿å­˜åˆ°ç¼“å­˜
                url_cache['subject_details'][subject_id] = result
                url_cache['subject_types'][subject_id] = media_type
                
                return result
                
            else:
                print(f"è·å–æ¡ç›® {subject_id} å¤±è´¥: HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                    time.sleep(3)
                    continue
                else:
                    raise requests.RequestException(f"è·å–æ¡ç›®ä¿¡æ¯å¤±è´¥: HTTP {response.status_code}")
                    
        except Exception as e:
            print(f"è·å–æ¡ç›® {subject_id} æ—¶å‡ºé”™: {e}")
            if attempt < max_retries - 1:
                print(f"å°†åœ¨ 3 ç§’åé‡è¯•...")
                time.sleep(3)
                continue
            else:
                raise
    
    return None

def process_status(status, cookie, early_check=False):
    """å¤„ç†å•æ¡å¹¿æ’­ï¼Œå°è¯•æå–ç”µå½±/å‰§é›†ä¿¡æ¯
    
    å½“early_checkä¸ºTrueæ—¶ï¼Œåªæå–subject_idå¹¶è¿”å›ï¼Œä¸è·å–è¯¦ç»†ä¿¡æ¯
    """
    try:
        # è‡ªå®šä¹‰è¯·æ±‚å¤´
        headers = make_douban_headers(cookie)
        
        # æŸ¥æ‰¾å¯èƒ½çš„ç”µå½±/å‰§é›†é“¾æ¥
        subject_info = None
        subject_id = None
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”µå½±/å‰§é›†é“¾æ¥
        if 'block-subject' in str(status):
            # ä»åˆ†äº«çš„ç”µå½±/å‰§é›†å¡ç‰‡ä¸­æå–
            subject_block = status.find('div', class_='block-subject')
            if subject_block:
                subject_link = subject_block.find('a', href=True)
                if subject_link:
                    subject_url = subject_link['href']
                    subject_id = extract_subject_id(subject_url)
                    if subject_id and early_check:
                        return {"id": subject_id, "title": subject_link.text.strip()}
                    elif subject_id:
                        # è·å–å®Œæ•´ç”µå½±ä¿¡æ¯
                        subject_info = get_subject_details(subject_id, cookie)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„å‘Šç‰‡é“¾æ¥
        elif 'block-video' in str(status):
            # ä»é¢„å‘Šç‰‡ä¸­æå–
            video_block = status.find('div', class_='block-video')
            if video_block:
                # è·å–é¢„å‘Šç‰‡ID
                video_player = video_block.find('div', class_='video-player')
                if video_player:
                    # å°è¯•ä»onclickæˆ–å…¶ä»–å±æ€§ä¸­æ‰¾åˆ°é¢„å‘Šç‰‡ID
                    match = re.search(r'/trailer/(\d+)', str(status))
                    if match:
                        trailer_id = match.group(1)
                        if early_check:
                            # ä»…è·å–åŸºæœ¬ä¿¡æ¯ä»¥æ£€æŸ¥ID
                            subject_info_from_trailer = get_subject_info_from_trailer(trailer_id, cookie)
                            if subject_info_from_trailer:
                                return {"id": subject_info_from_trailer['id'], "title": subject_info_from_trailer['title']}
                        else:
                            subject_info_from_trailer = get_subject_info_from_trailer(trailer_id, cookie)
                            if subject_info_from_trailer:
                                subject_id = subject_info_from_trailer['id']
                                if subject_id:
                                    # è·å–å®Œæ•´ç”µå½±ä¿¡æ¯
                                    subject_info = get_subject_details(subject_id, cookie)
        
        # æŸ¥æ‰¾çº¯æ–‡æœ¬å½¢å¼çš„é“¾æ¥
        if not subject_info and not (early_check and subject_id):
            links = status.find_all('a', href=True)
            for link in links:
                url = link['href']
                
                # å¤„ç†è±†ç“£é“¾æ¥
                if 'movie.douban.com/subject/' in url:
                    subject_id = extract_subject_id(url)
                    if subject_id and early_check:
                        return {"id": subject_id, "title": link.text.strip()}
                    elif subject_id:
                        # è·å–å®Œæ•´ç”µå½±ä¿¡æ¯
                        subject_info = get_subject_details(subject_id, cookie)
                        break
                
                # å¤„ç†é¢„å‘Šç‰‡é“¾æ¥
                elif 'movie.douban.com/trailer/' in url:
                    trailer_id = extract_trailer_id(url)
                    if trailer_id:
                        if early_check:
                            subject_info_from_trailer = get_subject_info_from_trailer(trailer_id, cookie)
                            if subject_info_from_trailer:
                                return {"id": subject_info_from_trailer['id'], "title": subject_info_from_trailer['title']}
                        else:
                            subject_info_from_trailer = get_subject_info_from_trailer(trailer_id, cookie)
                            if subject_info_from_trailer:
                                subject_id = subject_info_from_trailer['id']
                                if subject_id:
                                    # è·å–å®Œæ•´ç”µå½±ä¿¡æ¯
                                    subject_info = get_subject_details(subject_id, cookie)
                                    break
                
                # å¤„ç†çŸ­é“¾æ¥ï¼ˆdouc.ccï¼‰
                elif 'douc.cc' in url and not early_check:  # æ—©æœŸæ£€æŸ¥è·³è¿‡çŸ­é“¾æ¥å¤„ç†
                    print(f"å‘ç°çŸ­é“¾æ¥: {url}")
                    actual_url = get_redirect_url(url, headers)
                    print(f"çŸ­é“¾æ¥é‡å®šå‘åˆ°: {actual_url}")
                    
                    if actual_url:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç”µå½±/å‰§é›†é¡µé¢
                        if 'movie.douban.com/subject/' in actual_url:
                            subject_id = extract_subject_id(actual_url)
                            if subject_id:
                                # è·å–å®Œæ•´ç”µå½±ä¿¡æ¯
                                subject_info = get_subject_details(subject_id, cookie)
                                break
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„å‘Šç‰‡é¡µé¢
                        elif 'movie.douban.com/trailer/' in actual_url:
                            trailer_id = extract_trailer_id(actual_url)
                            if trailer_id:
                                subject_info_from_trailer = get_subject_info_from_trailer(trailer_id, cookie)
                                if subject_info_from_trailer:
                                    subject_id = subject_info_from_trailer['id']
                                    if subject_id:
                                        # è·å–å®Œæ•´ç”µå½±ä¿¡æ¯
                                        subject_info = get_subject_details(subject_id, cookie)
                                        break
        
        if early_check and subject_id:
            return {"id": subject_id, "title": "æœªçŸ¥æ ‡é¢˜"}
        
        if subject_info and subject_id:
            # è·å–å‘å¸ƒæ—¥æœŸ
            date_elem = status.find('span', class_='created_at')
            date = date_elem.text.strip() if date_elem else ''
            
            # è·å–å¹¿æ’­IDå’Œå†…å®¹
            status_id = ''
            status_div = status.find('div', {'data-status-id': True})
            if status_div:
                status_id = status_div['data-status-id']
            
            # è·å–å¹¿æ’­å†…å®¹
            content = ''
            blockquote = status.find('blockquote')
            if blockquote:
                p = blockquote.find('p')
                if p:
                    content = p.text.strip()
            
            # æ·»åŠ å¹¿æ’­ç‰¹æœ‰çš„ä¿¡æ¯
            subject_info['date_added'] = date
            subject_info['status_id'] = status_id
            subject_info['status_content'] = content
            subject_info['notified'] = False
            
            return subject_info
    
    except Exception as e:
        print(f"å¤„ç†å¹¿æ’­æ—¶å‡ºé”™: {e}")
    
    return None

def get_douban_status_html(user_id, cookie, pages=1):
    """è·å–è±†ç“£ç”¨æˆ·å¹¿æ’­çš„HTMLå†…å®¹"""
    all_html = ""
    page_count = 0
    
    # ä½¿ç”¨douban_utilsç”Ÿæˆè¯·æ±‚å¤´
    headers = make_douban_headers(cookie)
    
    try:
        for page in range(1, pages + 1):
            url = f"https://www.douban.com/people/{user_id}/statuses?p={page}"
            print(f"è·å–ç¬¬ {page}/{pages} é¡µå¹¿æ’­")
            
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                all_html += response.text
                page_count += 1
                
                # åœ¨è·å–å¤šé¡µæ—¶æ·»åŠ éšæœºå»¶è¿Ÿ
                if page < pages:
                    delay = random.uniform(1, 3)  # 1-3ç§’éšæœºå»¶è¿Ÿ
                    print(f"ç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)
            else:
                print(f"è·å–ç¬¬ {page} é¡µå¤±è´¥: HTTPçŠ¶æ€ç  {response.status_code}")
                break
                
        print(f"æˆåŠŸè·å–äº† {page_count} é¡µå¹¿æ’­")
        return all_html
    except Exception as e:
        print(f"è·å–å¹¿æ’­æ—¶å‡ºé”™: {e}")
        return ""

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    # ä½¿ç”¨ä»utilså¯¼å…¥çš„load_configå‡½æ•°ï¼Œæ·»åŠ åˆ«åé¿å…é€’å½’
    from src.utils.douban_utils import load_config as load_config_from_utils
    return load_config_from_utils()

def load_all_status_data():
    """åŠ è½½æ‰€æœ‰å¹¿æ’­æ•°æ®"""
    return load_json_data(STATUS_FILE, {})

def save_all_status_data(all_data):
    """ä¿å­˜æ‰€æœ‰å¹¿æ’­æ•°æ®åˆ°æ–‡ä»¶"""
    save_json_data(all_data, STATUS_FILE)

def is_duplicate(subject_id, user_id, all_data):
    """æ£€æŸ¥æ¡ç›®æ˜¯å¦é‡å¤"""
    user_data = all_data.get(user_id, {'movies': [], 'tv_shows': []})
    all_items = user_data['movies'] + user_data['tv_shows']
    
    for item in all_items:
        if subject_id == item.get('id'):
            return True
    return False

def parse_status_html(html_content, user_id, all_data, cookie):
    """è§£æHTMLå†…å®¹å¹¶æå–å¹¿æ’­ä¸­çš„ç”µå½±/å‰§é›†"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # è·å–æˆ–åˆå§‹åŒ–ç”¨æˆ·æ•°æ®
    user_data = all_data.get(user_id, {'movies': [], 'tv_shows': []})
    movies = user_data['movies']
    tv_shows = user_data['tv_shows']
    
    # æ‰¾åˆ°å¹¿æ’­åˆ—è¡¨
    status_items = soup.find_all('div', class_='status-wrapper')
    total_items = len(status_items)
    new_items = 0
    
    print(f"\nå¼€å§‹å¤„ç† {total_items} æ¡å¹¿æ’­...")
    
    for index, status in enumerate(status_items, 1):
        try:
            # å…ˆè¿›è¡Œå¿«é€Ÿæ£€æŸ¥ï¼Œåªæå–æ¡ç›®IDå’Œç®€å•æ ‡é¢˜
            subject_basic = process_status(status, cookie, early_check=True)
            
            if subject_basic and subject_basic['id']:
                # æ£€æŸ¥æ˜¯å¦é‡å¤
                if is_duplicate(subject_basic['id'], user_id, all_data):
                    print(f"\næ¡ç›®å·²å­˜åœ¨ï¼Œè·³è¿‡: {subject_basic['title']}")
                    continue  # è·³è¿‡ä¸‹é¢çš„å¤„ç†ï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€æ¬¡å¾ªç¯ï¼Œä¸æ·»åŠ ä»»ä½•å»¶è¿Ÿ
                
                # æ¡ç›®ä¸é‡å¤ï¼Œè·å–å®Œæ•´ä¿¡æ¯
                subject = process_status(status, cookie)
                
                if subject and subject['id']:
                    print(f"\nå¤„ç†æ–°æ¡ç›®: {subject['title']}")
                    new_items += 1
                    
                    # è®¾ç½®notifiedä¸ºFalseï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæ–°æ¡ç›®éœ€è¦é€šçŸ¥
                    subject['notified'] = False
                    
                    # æ ¹æ®ç±»å‹æ·»åŠ åˆ°å¯¹åº”åˆ—è¡¨
                    if subject["type"] == "movie":
                        movies.append(subject)
                        print(f"å·²æ·»åŠ åˆ°ç”µå½±åˆ—è¡¨: {subject['title']}")
                    else:
                        tv_shows.append(subject)
                        print(f"å·²æ·»åŠ åˆ°ç”µè§†å‰§åˆ—è¡¨: {subject['title']}")
                    
                    # å®æ—¶ä¿å­˜æ•°æ®
                    all_data[user_id] = {'movies': movies, 'tv_shows': tv_shows}
                    save_all_status_data(all_data)
                    print("æ•°æ®å·²ä¿å­˜")
                    
                    # åªåœ¨æˆåŠŸå¤„ç†æ–°æ¡ç›®åæ·»åŠ å»¶è¿Ÿ
                    if index < total_items:
                        delay = random.uniform(3, 7)
                        print(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                        time.sleep(delay)
                
        except Exception as e:
            print(f"å¤„ç†å¹¿æ’­æ¡ç›®æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nå¤„ç†å®Œæˆ! å‘ç° {new_items} ä¸ªæ–°æ¡ç›®")
    print(f"å½“å‰ç”¨æˆ·å…±æœ‰ {len(movies)} éƒ¨ç”µå½±, {len(tv_shows)} éƒ¨ç”µè§†å‰§")
    
    # æ·»åŠ æ›´æ–°æ—¶é—´å’Œæ–°æ¡ç›®æ•°é‡
    result = {
        'movies': movies,
        'tv_shows': tv_shows,
        'update_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'new_items_count': new_items
    }
    
    return result

def send_telegram_message(message, config, has_new_content=False):
    """å‘é€Telegramæ¶ˆæ¯ï¼Œè¿™æ˜¯ä¸€ä¸ªè½¬å‘å‡½æ•°"""
    # ä½¿ç”¨ä»utilså¯¼å…¥çš„send_telegram_messageå‡½æ•°ï¼Œæ·»åŠ åˆ«åé¿å…é€’å½’
    from src.utils.douban_utils import send_telegram_message as send_telegram_message_from_utils
    send_telegram_message_from_utils(message, config, has_new_content)

def send_wecom_message(message, config, has_new_content=False):
    """å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯ï¼Œè¿™æ˜¯ä¸€ä¸ªè½¬å‘å‡½æ•°"""
    from src.utils.douban_utils import send_wecom_message as send_wecom_message_from_utils
    send_wecom_message_from_utils(message, config, has_new_content)

def cleanup_temp_files():
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    # æ¸…ç†å…¶ä»–å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
    pass

def load_url_cache():
    """åŠ è½½URLç¼“å­˜"""
    global URL_CACHE
    URL_CACHE = load_json_data(URL_CACHE_FILE, {})
    print(f"å·²åŠ è½½ {len(URL_CACHE)} æ¡URLç¼“å­˜")

def save_url_cache():
    """ä¿å­˜URLç¼“å­˜"""
    try:
        save_json_data(URL_CACHE, URL_CACHE_FILE)
        print(f"å·²ä¿å­˜ {len(URL_CACHE)} æ¡URLç¼“å­˜")
    except Exception as e:
        print(f"ä¿å­˜URLç¼“å­˜å¤±è´¥: {e}")

def fetch_user_status(user_id, cookie, pages=1):
    """è·å–ç”¨æˆ·å¹¿æ’­çŠ¶æ€å¹¶å¤„ç†"""
    try:
        # åŠ è½½ç°æœ‰æ•°æ®
        all_data = load_all_status_data()
        
        # è·å–å¹¿æ’­HTMLå†…å®¹
        html_content = get_douban_status_html(user_id, cookie, pages)
        
        if not html_content:
            print(f"æœªèƒ½è·å–ç”¨æˆ· {user_id} çš„å¹¿æ’­å†…å®¹")
            return False
            
        # è§£æHTMLå¹¶æå–ç”µå½±/å‰§é›†
        result = parse_status_html(html_content, user_id, all_data, cookie)
        
        # æ›´æ–°ç”¨æˆ·æ•°æ®
        all_data[user_id] = result
        save_all_status_data(all_data)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹
        return result.get('new_items_count', 0) > 0
        
    except Exception as e:
        print(f"è·å–ç”¨æˆ· {user_id} å¹¿æ’­çŠ¶æ€æ—¶å‡ºé”™: {e}")
        return False

def main():
    try:
        # åœ¨å¼€å§‹æ—¶è¿›è¡Œç¼“å­˜è¿ç§»
        print("æ£€æŸ¥å¹¶è¿ç§»æ—§ç¼“å­˜æ•°æ®...")
        migrate_legacy_cache_data()
        
        # åŠ è½½URLç¼“å­˜
        load_url_cache()
        
        config = load_config()
        cookie = config.get('cookie', '')
        
        # éªŒè¯ cookie
        if not cookie:
            message = "âŒ Cookie æœªé…ç½®ï¼Œè¯·å…ˆé…ç½® Cookie"
            print(message)
            send_telegram_message(message, config, False)
            send_wecom_message(message, config, False)
            return
            
        if not check_cookie_valid(cookie):
            message = "âŒ Cookie å·²å¤±æ•ˆï¼Œè¯·æ›´æ–° Cookie"
            print(message)
            send_telegram_message(message, config, False)
            send_wecom_message(message, config, False)
            return
        
        print("\nå¼€å§‹è·å–è±†ç“£å¹¿æ’­æ•°æ®...")
        
        # è®°å½•æ˜¯å¦æœ‰ä»»ä½•ç”¨æˆ·æ•°æ®æ›´æ–°
        any_updates = False
        
        # å¤„ç†æ¯ä¸ªç”¨æˆ·çš„æ•°æ®
        statuses = config.get('statuses', [])
        total_users = len(statuses)
        
        for i, status in enumerate(statuses, 1):
            user_id = status['id']
            note = status.get('note', '')
            pages = status.get('pages', 1)
            try:
                print(f"\n[{i}/{total_users}] å¤„ç†ç”¨æˆ·å¹¿æ’­: {note or user_id}")
                
                # è·å–æ›´æ–°
                has_updates = fetch_user_status(user_id, cookie, pages)
                
                if has_updates:
                    any_updates = True
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªç”¨æˆ·ï¼Œæ·»åŠ éšæœºå»¶è¿Ÿ
                if i < total_users:
                    delay = random.uniform(1, 3)  # 1-3ç§’çš„éšæœºå»¶è¿Ÿ
                    print(f"ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªç”¨æˆ·...")
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"å¤„ç†ç”¨æˆ·å¹¿æ’­ {user_id} æ—¶å‡ºé”™: {e}")
                continue
        
        # é‡æ–°åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œç”¨äºç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        all_data = load_all_status_data()
        
        # ç»Ÿè®¡æ€»æ•°å’Œæ–°å¢å†…å®¹
        total_movies = 0
        total_tv_shows = 0
        new_movies = []
        new_tv_shows = []
        
        # éå†æ‰€æœ‰ç”¨æˆ·æ•°æ®ï¼Œç»Ÿè®¡æ€»æ•°å’ŒæŸ¥æ‰¾æœªé€šçŸ¥çš„æ–°é¡¹ç›®
        for user_id, user_data in all_data.items():
            # ç»Ÿè®¡æ€»æ•°
            total_movies += len(user_data.get('movies', []))
            total_tv_shows += len(user_data.get('tv_shows', []))
            
            # æŸ¥æ‰¾æœªé€šçŸ¥çš„é¡¹ç›®
            for movie in user_data.get('movies', []):
                if not movie.get('notified', True):
                    new_movies.append(movie)
            
            for tv in user_data.get('tv_shows', []):
                if not tv.get('notified', True):
                    new_tv_shows.append(tv)
        
        # æ–°å¢æ•°é‡
        new_movies_count = len(new_movies)
        new_tv_shows_count = len(new_tv_shows)
        has_new_content = new_movies_count > 0 or new_tv_shows_count > 0
        
        # æ„å»ºé€šçŸ¥æ¶ˆæ¯
        message = (
            f"ğŸ¬ *è±†ç“£å¹¿æ’­æ›´æ–°å®Œæˆ*\n\n"
        )
        
        # æ ¹æ®å®é™…æ–°å¢æ•°é‡å±•ç¤ºæ¶ˆæ¯
        if not has_new_content:
            # æ²¡æœ‰æ–°å†…å®¹æ—¶çš„æ¶ˆæ¯
            message += "âš ï¸ æœ¬æ¬¡æ›´æ–°æ²¡æœ‰å‘ç°æ–°çš„å†…å®¹ã€‚\n\n"
        else:
            message += f"ğŸ“Š æœ¬æ¬¡æ–°å¢: {new_movies_count} éƒ¨ç”µå½±, {new_tv_shows_count} éƒ¨å‰§é›†\n\n"
        
        # æ›´æ–°æ—¶é—´å’Œæ€»è®¡æ•°
        update_time = time.strftime('%Y-%m-%d %H:%M:%S')
        message += (
            f"æ›´æ–°æ—¶é—´: {update_time}\n"
            f"æ€»ç”µå½±æ•°: {total_movies} éƒ¨\n"
            f"æ€»å‰§é›†æ•°: {total_tv_shows} éƒ¨\n\n"
        )
        
        # æ·»åŠ æ–°æ›´æ–°çš„ç”µå½±ä¿¡æ¯
        if has_new_content:
            if new_movies:
                message += "*æ–°å¢ç”µå½±:*\n"
                for movie in new_movies:  # æ˜¾ç¤ºæ‰€æœ‰æ–°ç”µå½±ï¼Œä¸é™åˆ¶æ•°é‡
                    movie_link = movie.get('url', f"https://movie.douban.com/subject/{movie.get('id', '')}/")
                    rating = movie.get('rating', '')
                    rating_text = f" - â­{rating}" if rating else ""
                    message += f"â€¢ <a href='{movie_link}'>{movie['title']}</a>{rating_text}\n"
                    movie['notified'] = True  # æ ‡è®°ä¸ºå·²é€šçŸ¥
                
                message += "\n"
            
            # å¦‚æœæœ‰æ–°å¢å‰§é›†
            if new_tv_shows:
                message += "*æ–°å¢å‰§é›†:*\n"
                for tv in new_tv_shows:  # æ˜¾ç¤ºæ‰€æœ‰æ–°å‰§é›†ï¼Œä¸é™åˆ¶æ•°é‡
                    tv_link = tv.get('url', f"https://movie.douban.com/subject/{tv.get('id', '')}/")
                    rating = tv.get('rating', '')
                    rating_text = f" - â­{rating}" if rating else ""
                    message += f"â€¢ <a href='{tv_link}'>{tv['title']}</a>{rating_text}\n"
                    tv['notified'] = True  # æ ‡è®°ä¸ºå·²é€šçŸ¥
                
                message += "\n"
            
            # ä¿å­˜å·²æ ‡è®°çš„æ•°æ®
            save_all_status_data(all_data)
        
        # å‘é€é€šçŸ¥æ¶ˆæ¯
        send_telegram_message(message, config, has_new_content)
        send_wecom_message(message, config, has_new_content)
        
        print("\nå¹¿æ’­æ•°æ®è·å–å®Œæˆï¼")
        
    except Exception as e:
        error_message = f"âŒ è·å–è±†ç“£å¹¿æ’­æ•°æ®æ—¶å‡ºé”™: {str(e)}"
        print(error_message)
        send_telegram_message(error_message, config, False)
        send_wecom_message(error_message, config, False)
    finally:
        # ä¿å­˜URLç¼“å­˜
        save_url_cache()
        
        # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        cleanup_temp_files()

if __name__ == "__main__":
    main() 